{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:436.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # For ReLU, etc.\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "# Force PyTorch to use float32 as default type\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping joker - couldn't identify rank or suit\n",
      "Loaded 7509 images with shape (7509, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "training_data_dir = os.path.join(\"large-dataset\", \"train\")\n",
    "test_data_dir = os.path.join(\"large-dataset\", \"test\")\n",
    "\n",
    "\n",
    "# Define mappings for ranks and suits\n",
    "rank_map = {\n",
    "    'ace': 0,\n",
    "    '2': 1, 'two': 1,\n",
    "    '3': 2, 'three': 2,\n",
    "    '4': 3, 'four': 3,\n",
    "    '5': 4, 'five': 4,\n",
    "    '6': 5, 'six': 5,\n",
    "    '7': 6, 'seven': 6,\n",
    "    '8': 7, 'eight': 7,\n",
    "    '9': 8, 'nine': 8,\n",
    "    '10': 9, 'ten': 9,\n",
    "    'jack': 10,\n",
    "    'queen': 11,\n",
    "    'king': 12,\n",
    "}\n",
    "\n",
    "suit_map = {\n",
    "    'clubs': 0,\n",
    "    'diamonds': 1,\n",
    "    'hearts': 2,\n",
    "    'spades': 3,\n",
    "}\n",
    "\n",
    "# Collect images and labels\n",
    "X, ranks, suits = [], [], []\n",
    "\n",
    "card_dirs = [d for d in glob(os.path.join(training_data_dir, '*')) if os.path.isdir(d)]\n",
    "target_size = (224, 224)\n",
    "\n",
    "for card_dir in card_dirs:\n",
    "    card_name = os.path.basename(card_dir).lower()\n",
    "    \n",
    "    # Identify rank and suit from directory name\n",
    "    found_rank = None\n",
    "    found_suit = None\n",
    "    \n",
    "    \n",
    "    # Look for rank\n",
    "    for rank_name, rank_idx in rank_map.items():\n",
    "        if rank_name in card_name and rank_name != 'joker':\n",
    "            found_rank = rank_idx\n",
    "            break\n",
    "    \n",
    "    # Look for suit\n",
    "    for suit_name, suit_idx in suit_map.items():\n",
    "        if suit_name in card_name and suit_name != 'joker':\n",
    "            found_suit = suit_idx\n",
    "            break\n",
    "    \n",
    "    if found_rank is None or found_suit is None:\n",
    "        print(f\"Skipping {card_name} - couldn't identify rank or suit\")\n",
    "        continue\n",
    "    \n",
    "    # Process all images in this folder\n",
    "    image_paths = glob(os.path.join(card_dir, '*.jpg')) + glob(os.path.join(card_dir, '*.png'))\n",
    "    \n",
    "    for path in image_paths:\n",
    "        if os.path.exists(path):\n",
    "            img = cv2.imread(path)\n",
    "            if img is not None:\n",
    "                # Convert to RGB and resize\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, target_size)\n",
    "                \n",
    "                X.append(img)\n",
    "                ranks.append(found_rank)\n",
    "                suits.append(found_suit)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X, dtype=np.float32) / 255.0\n",
    "ranks = np.array(ranks, dtype=np.int64)\n",
    "suits = np.array(suits, dtype=np.int64)\n",
    "\n",
    "print(f\"Loaded {len(X)} images with shape {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardDataset(Dataset):\n",
    "    def __init__(self, images, ranks, suits, transform=None):\n",
    "        self.images = images\n",
    "        self.ranks = ranks\n",
    "        self.suits = suits\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # shape (H, W, C), range [0,1]\n",
    "        \n",
    "        # If we have a transform pipeline that expects a PIL image,\n",
    "        # we can convert here.\n",
    "        if self.transform:\n",
    "            import torchvision.transforms.functional as TF\n",
    "            # Convert from NumPy array to PIL Image\n",
    "            image = TF.to_pil_image(image)  \n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default: convert to tensor and permute to C,H,W\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n",
    "        \n",
    "        return (\n",
    "            image,\n",
    "            torch.tensor(self.ranks[idx], dtype=torch.long),\n",
    "            torch.tensor(self.suits[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 6007 images\n",
      "Val set: 1502 images\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, ranks_train, ranks_val, suits_train, suits_val = train_test_split(\n",
    "    X, ranks, suits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train set:\", len(X_train), \"images\")\n",
    "print(\"Val set:\", len(X_val), \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),      # Resize\n",
    "    transforms.RandomHorizontalFlip(),  # 50% chance flip\n",
    "    transforms.RandomRotation(180),      # Rotate up to ±180 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# No augmentations for validation (only resize + normalize)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = CardDataset(X_train, ranks_train, suits_train, transform=train_transform)\n",
    "val_dataset   = CardDataset(X_val,   ranks_val,   suits_val,   transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CardClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # For 224x224 input, after 3 pool layers => spatial size is 16x16\n",
    "        self.flat_features = 64 * 28 * 28\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flat_features, 224)\n",
    "        \n",
    "        # Separate outputs for rank and suit\n",
    "        self.rank_classifier = nn.Linear(224, 13)  \n",
    "        self.suit_classifier = nn.Linear(224, 4)   \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        rank_output = self.rank_classifier(x)\n",
    "        suit_output = self.suit_classifier(x)\n",
    "        \n",
    "        return rank_output, suit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CardClassifier().to(device)\n",
    "\n",
    "# Double-check all model parameters are float32\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.float()\n",
    "\n",
    "criterion_rank = nn.CrossEntropyLoss()\n",
    "criterion_suit = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Gradient accumulation function\n",
    "def train_epoch(model, loader, optimizer, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (images, ranks_batch, suits_batch) in enumerate(loader):\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        ranks_batch = ranks_batch.to(device)\n",
    "        suits_batch = suits_batch.to(device)\n",
    "        \n",
    "        rank_outputs, suit_outputs = model(images)\n",
    "        rank_loss = criterion_rank(rank_outputs, ranks_batch)\n",
    "        suit_loss = criterion_suit(suit_outputs, suits_batch)\n",
    "        \n",
    "        loss = rank_loss + suit_loss\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        batch_count += 1\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Batch {i+1}/{len(loader)}, Loss: {loss.item()*accumulation_steps:.4f}\")\n",
    "    \n",
    "    return total_loss / batch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 started\n",
      "  Batch 10/188, Loss: 5.7883\n",
      "  Batch 20/188, Loss: 4.0914\n",
      "  Batch 30/188, Loss: 4.0287\n",
      "  Batch 40/188, Loss: 3.8701\n",
      "  Batch 50/188, Loss: 3.9099\n",
      "  Batch 60/188, Loss: 3.8446\n",
      "  Batch 70/188, Loss: 3.7425\n",
      "  Batch 80/188, Loss: 3.6646\n",
      "  Batch 90/188, Loss: 3.5791\n",
      "  Batch 100/188, Loss: 3.5086\n",
      "  Batch 110/188, Loss: 3.4837\n",
      "  Batch 120/188, Loss: 3.3679\n",
      "  Batch 130/188, Loss: 3.4633\n",
      "  Batch 140/188, Loss: 3.7583\n",
      "  Batch 150/188, Loss: 3.5371\n",
      "  Batch 160/188, Loss: 3.3744\n",
      "  Batch 170/188, Loss: 3.4931\n",
      "  Batch 180/188, Loss: 3.3276\n",
      "Epoch 1 Loss: 3.7879\n",
      "  Model saved (improved loss: 3.7879)\n",
      "Epoch 2/100 started\n",
      "  Batch 10/188, Loss: 3.5911\n",
      "  Batch 20/188, Loss: 3.1769\n",
      "  Batch 30/188, Loss: 3.0839\n",
      "  Batch 40/188, Loss: 3.1905\n",
      "  Batch 50/188, Loss: 3.4016\n",
      "  Batch 60/188, Loss: 3.2000\n",
      "  Batch 70/188, Loss: 3.3788\n",
      "  Batch 80/188, Loss: 3.3118\n",
      "  Batch 90/188, Loss: 3.0750\n",
      "  Batch 100/188, Loss: 3.0651\n",
      "  Batch 110/188, Loss: 3.0165\n",
      "  Batch 120/188, Loss: 3.0451\n",
      "  Batch 130/188, Loss: 2.9643\n",
      "  Batch 140/188, Loss: 2.7641\n",
      "  Batch 150/188, Loss: 3.2891\n",
      "  Batch 160/188, Loss: 3.0219\n",
      "  Batch 170/188, Loss: 2.9836\n",
      "  Batch 180/188, Loss: 3.1873\n",
      "Epoch 2 Loss: 3.1427\n",
      "  Model saved (improved loss: 3.1427)\n",
      "Epoch 3/100 started\n",
      "  Batch 10/188, Loss: 2.7778\n",
      "  Batch 20/188, Loss: 2.6931\n",
      "  Batch 30/188, Loss: 3.4429\n",
      "  Batch 40/188, Loss: 2.7057\n",
      "  Batch 50/188, Loss: 2.5974\n",
      "  Batch 60/188, Loss: 2.4456\n",
      "  Batch 70/188, Loss: 2.7307\n",
      "  Batch 80/188, Loss: 2.9252\n",
      "  Batch 90/188, Loss: 2.5352\n",
      "  Batch 100/188, Loss: 2.8650\n",
      "  Batch 110/188, Loss: 3.2814\n",
      "  Batch 120/188, Loss: 2.6098\n",
      "  Batch 130/188, Loss: 2.8941\n",
      "  Batch 140/188, Loss: 2.6766\n",
      "  Batch 150/188, Loss: 2.5570\n",
      "  Batch 160/188, Loss: 2.6123\n",
      "  Batch 170/188, Loss: 2.7507\n",
      "  Batch 180/188, Loss: 2.9707\n",
      "Epoch 3 Loss: 2.8634\n",
      "  Model saved (improved loss: 2.8634)\n",
      "Epoch 4/100 started\n",
      "  Batch 10/188, Loss: 2.3899\n",
      "  Batch 20/188, Loss: 2.8747\n",
      "  Batch 30/188, Loss: 2.4842\n",
      "  Batch 40/188, Loss: 2.7653\n",
      "  Batch 50/188, Loss: 2.7749\n",
      "  Batch 60/188, Loss: 3.3054\n",
      "  Batch 70/188, Loss: 2.5386\n",
      "  Batch 80/188, Loss: 2.5851\n",
      "  Batch 90/188, Loss: 2.8332\n",
      "  Batch 100/188, Loss: 2.6030\n",
      "  Batch 110/188, Loss: 2.5210\n",
      "  Batch 120/188, Loss: 2.8137\n",
      "  Batch 130/188, Loss: 3.0000\n",
      "  Batch 140/188, Loss: 2.5424\n",
      "  Batch 150/188, Loss: 2.5484\n",
      "  Batch 160/188, Loss: 2.5233\n",
      "  Batch 170/188, Loss: 2.5970\n",
      "  Batch 180/188, Loss: 3.0099\n",
      "Epoch 4 Loss: 2.7065\n",
      "  Model saved (improved loss: 2.7065)\n",
      "Epoch 5/100 started\n",
      "  Batch 10/188, Loss: 3.0855\n",
      "  Batch 20/188, Loss: 2.3091\n",
      "  Batch 30/188, Loss: 1.9547\n",
      "  Batch 40/188, Loss: 2.6228\n",
      "  Batch 50/188, Loss: 3.3011\n",
      "  Batch 60/188, Loss: 2.1308\n",
      "  Batch 70/188, Loss: 2.8080\n",
      "  Batch 80/188, Loss: 2.7819\n",
      "  Batch 90/188, Loss: 2.6545\n",
      "  Batch 100/188, Loss: 2.6854\n",
      "  Batch 110/188, Loss: 2.6357\n",
      "  Batch 120/188, Loss: 2.8258\n",
      "  Batch 130/188, Loss: 2.2897\n",
      "  Batch 140/188, Loss: 2.5013\n",
      "  Batch 150/188, Loss: 2.4113\n",
      "  Batch 160/188, Loss: 2.0912\n",
      "  Batch 170/188, Loss: 2.1587\n",
      "  Batch 180/188, Loss: 2.6828\n",
      "Epoch 5 Loss: 2.6106\n",
      "  Model saved (improved loss: 2.6106)\n",
      "Epoch 6/100 started\n",
      "  Batch 10/188, Loss: 2.5775\n",
      "  Batch 20/188, Loss: 2.7759\n",
      "  Batch 30/188, Loss: 2.5704\n",
      "  Batch 40/188, Loss: 2.6279\n",
      "  Batch 50/188, Loss: 2.2634\n",
      "  Batch 60/188, Loss: 2.0863\n",
      "  Batch 70/188, Loss: 2.4653\n",
      "  Batch 80/188, Loss: 2.6781\n",
      "  Batch 90/188, Loss: 2.4809\n",
      "  Batch 100/188, Loss: 2.6103\n",
      "  Batch 110/188, Loss: 3.0011\n",
      "  Batch 120/188, Loss: 2.9119\n",
      "  Batch 130/188, Loss: 2.2124\n",
      "  Batch 140/188, Loss: 2.6499\n",
      "  Batch 150/188, Loss: 2.7213\n",
      "  Batch 160/188, Loss: 2.7791\n",
      "  Batch 170/188, Loss: 2.6753\n",
      "  Batch 180/188, Loss: 2.4583\n",
      "Epoch 6 Loss: 2.5551\n",
      "  Model saved (improved loss: 2.5551)\n",
      "Epoch 7/100 started\n",
      "  Batch 10/188, Loss: 2.4723\n",
      "  Batch 20/188, Loss: 2.6296\n",
      "  Batch 30/188, Loss: 2.6979\n",
      "  Batch 40/188, Loss: 2.5020\n",
      "  Batch 50/188, Loss: 2.3640\n",
      "  Batch 60/188, Loss: 2.0602\n",
      "  Batch 70/188, Loss: 2.9962\n",
      "  Batch 80/188, Loss: 2.6389\n",
      "  Batch 90/188, Loss: 1.9837\n",
      "  Batch 100/188, Loss: 2.2757\n",
      "  Batch 110/188, Loss: 2.4689\n",
      "  Batch 120/188, Loss: 2.9369\n",
      "  Batch 130/188, Loss: 2.0939\n",
      "  Batch 140/188, Loss: 2.6810\n",
      "  Batch 150/188, Loss: 2.3963\n",
      "  Batch 160/188, Loss: 2.3396\n",
      "  Batch 170/188, Loss: 2.6863\n",
      "  Batch 180/188, Loss: 2.7747\n",
      "Epoch 7 Loss: 2.4784\n",
      "  Model saved (improved loss: 2.4784)\n",
      "Epoch 8/100 started\n",
      "  Batch 10/188, Loss: 1.9794\n",
      "  Batch 20/188, Loss: 2.5253\n",
      "  Batch 30/188, Loss: 2.5636\n",
      "  Batch 40/188, Loss: 2.2757\n",
      "  Batch 50/188, Loss: 3.2149\n",
      "  Batch 60/188, Loss: 2.4084\n",
      "  Batch 70/188, Loss: 2.2325\n",
      "  Batch 80/188, Loss: 2.9557\n",
      "  Batch 90/188, Loss: 2.3966\n",
      "  Batch 100/188, Loss: 2.2929\n",
      "  Batch 110/188, Loss: 2.6537\n",
      "  Batch 120/188, Loss: 2.5077\n",
      "  Batch 130/188, Loss: 2.4593\n",
      "  Batch 140/188, Loss: 2.4164\n",
      "  Batch 150/188, Loss: 2.3091\n",
      "  Batch 160/188, Loss: 3.2182\n",
      "  Batch 170/188, Loss: 2.8372\n",
      "  Batch 180/188, Loss: 2.0828\n",
      "Epoch 8 Loss: 2.4690\n",
      "  Model saved (improved loss: 2.4690)\n",
      "Epoch 9/100 started\n",
      "  Batch 10/188, Loss: 2.4103\n",
      "  Batch 20/188, Loss: 2.1742\n",
      "  Batch 30/188, Loss: 2.4787\n",
      "  Batch 40/188, Loss: 2.4004\n",
      "  Batch 50/188, Loss: 2.2674\n",
      "  Batch 60/188, Loss: 2.4679\n",
      "  Batch 70/188, Loss: 2.7636\n",
      "  Batch 80/188, Loss: 2.5451\n",
      "  Batch 90/188, Loss: 2.4445\n",
      "  Batch 100/188, Loss: 2.1933\n",
      "  Batch 110/188, Loss: 2.2450\n",
      "  Batch 120/188, Loss: 2.2927\n",
      "  Batch 130/188, Loss: 2.3371\n",
      "  Batch 140/188, Loss: 2.5034\n",
      "  Batch 150/188, Loss: 2.0196\n",
      "  Batch 160/188, Loss: 2.2080\n",
      "  Batch 170/188, Loss: 2.5590\n",
      "  Batch 180/188, Loss: 2.5742\n",
      "Epoch 9 Loss: 2.3866\n",
      "  Model saved (improved loss: 2.3866)\n",
      "Epoch 10/100 started\n",
      "  Batch 10/188, Loss: 2.0023\n",
      "  Batch 20/188, Loss: 2.2493\n",
      "  Batch 30/188, Loss: 2.1358\n",
      "  Batch 40/188, Loss: 1.9269\n",
      "  Batch 50/188, Loss: 2.4709\n",
      "  Batch 60/188, Loss: 1.8516\n",
      "  Batch 70/188, Loss: 2.6373\n",
      "  Batch 80/188, Loss: 2.6155\n",
      "  Batch 90/188, Loss: 2.8803\n",
      "  Batch 100/188, Loss: 2.0642\n",
      "  Batch 110/188, Loss: 2.1042\n",
      "  Batch 120/188, Loss: 2.3864\n",
      "  Batch 130/188, Loss: 2.3399\n",
      "  Batch 140/188, Loss: 2.1636\n",
      "  Batch 150/188, Loss: 2.5794\n",
      "  Batch 160/188, Loss: 2.2533\n",
      "  Batch 170/188, Loss: 2.4160\n",
      "  Batch 180/188, Loss: 2.0977\n",
      "Epoch 10 Loss: 2.3660\n",
      "  Model saved (improved loss: 2.3660)\n",
      "Epoch 11/100 started\n",
      "  Batch 10/188, Loss: 2.0471\n",
      "  Batch 20/188, Loss: 2.4774\n",
      "  Batch 30/188, Loss: 1.9782\n",
      "  Batch 40/188, Loss: 2.6935\n",
      "  Batch 50/188, Loss: 2.4485\n",
      "  Batch 60/188, Loss: 2.8213\n",
      "  Batch 70/188, Loss: 2.0451\n",
      "  Batch 80/188, Loss: 1.9844\n",
      "  Batch 90/188, Loss: 2.5727\n",
      "  Batch 100/188, Loss: 2.9620\n",
      "  Batch 110/188, Loss: 3.0607\n",
      "  Batch 120/188, Loss: 2.5533\n",
      "  Batch 130/188, Loss: 2.1063\n",
      "  Batch 140/188, Loss: 2.7278\n",
      "  Batch 150/188, Loss: 2.2048\n",
      "  Batch 160/188, Loss: 2.0047\n",
      "  Batch 170/188, Loss: 2.4556\n",
      "  Batch 180/188, Loss: 2.2645\n",
      "Epoch 11 Loss: 2.3256\n",
      "  Model saved (improved loss: 2.3256)\n",
      "Epoch 12/100 started\n",
      "  Batch 10/188, Loss: 2.4618\n",
      "  Batch 20/188, Loss: 2.1876\n",
      "  Batch 30/188, Loss: 2.1612\n",
      "  Batch 40/188, Loss: 2.1463\n",
      "  Batch 50/188, Loss: 2.5789\n",
      "  Batch 60/188, Loss: 2.5038\n",
      "  Batch 70/188, Loss: 2.0244\n",
      "  Batch 80/188, Loss: 2.2891\n",
      "  Batch 90/188, Loss: 2.1751\n",
      "  Batch 100/188, Loss: 2.6454\n",
      "  Batch 110/188, Loss: 2.6485\n",
      "  Batch 120/188, Loss: 2.2867\n",
      "  Batch 130/188, Loss: 2.0933\n",
      "  Batch 140/188, Loss: 2.2623\n",
      "  Batch 150/188, Loss: 2.2777\n",
      "  Batch 160/188, Loss: 2.7774\n",
      "  Batch 170/188, Loss: 2.4320\n",
      "  Batch 180/188, Loss: 2.5620\n",
      "Epoch 12 Loss: 2.2990\n",
      "  Model saved (improved loss: 2.2990)\n",
      "Epoch 13/100 started\n",
      "  Batch 10/188, Loss: 2.1262\n",
      "  Batch 20/188, Loss: 2.2421\n",
      "  Batch 30/188, Loss: 2.4490\n",
      "  Batch 40/188, Loss: 1.9232\n",
      "  Batch 50/188, Loss: 2.4941\n",
      "  Batch 60/188, Loss: 2.0059\n",
      "  Batch 70/188, Loss: 1.8200\n",
      "  Batch 80/188, Loss: 2.0406\n",
      "  Batch 90/188, Loss: 1.8982\n",
      "  Batch 100/188, Loss: 2.1058\n",
      "  Batch 110/188, Loss: 2.5112\n",
      "  Batch 120/188, Loss: 2.0186\n",
      "  Batch 130/188, Loss: 2.0024\n",
      "  Batch 140/188, Loss: 2.1200\n",
      "  Batch 150/188, Loss: 2.3160\n",
      "  Batch 160/188, Loss: 2.0018\n",
      "  Batch 170/188, Loss: 2.4413\n",
      "  Batch 180/188, Loss: 2.4773\n",
      "Epoch 13 Loss: 2.2677\n",
      "  Model saved (improved loss: 2.2677)\n",
      "Epoch 14/100 started\n",
      "  Batch 10/188, Loss: 2.2145\n",
      "  Batch 20/188, Loss: 2.6062\n",
      "  Batch 30/188, Loss: 2.8431\n",
      "  Batch 40/188, Loss: 1.9634\n",
      "  Batch 50/188, Loss: 1.9817\n",
      "  Batch 60/188, Loss: 2.0001\n",
      "  Batch 70/188, Loss: 2.3858\n",
      "  Batch 80/188, Loss: 2.4515\n",
      "  Batch 90/188, Loss: 2.3304\n",
      "  Batch 100/188, Loss: 2.3264\n",
      "  Batch 110/188, Loss: 2.3434\n",
      "  Batch 120/188, Loss: 2.4476\n",
      "  Batch 130/188, Loss: 2.4089\n",
      "  Batch 140/188, Loss: 2.4736\n",
      "  Batch 150/188, Loss: 2.1300\n",
      "  Batch 160/188, Loss: 2.1335\n",
      "  Batch 170/188, Loss: 2.3595\n",
      "  Batch 180/188, Loss: 2.7945\n",
      "Epoch 14 Loss: 2.2473\n",
      "  Model saved (improved loss: 2.2473)\n",
      "Epoch 15/100 started\n",
      "  Batch 10/188, Loss: 2.4352\n",
      "  Batch 20/188, Loss: 2.4995\n",
      "  Batch 30/188, Loss: 1.8407\n",
      "  Batch 40/188, Loss: 1.7311\n",
      "  Batch 50/188, Loss: 2.0188\n",
      "  Batch 60/188, Loss: 2.1231\n",
      "  Batch 70/188, Loss: 2.4300\n",
      "  Batch 80/188, Loss: 2.7193\n",
      "  Batch 90/188, Loss: 1.9201\n",
      "  Batch 100/188, Loss: 2.5632\n",
      "  Batch 110/188, Loss: 2.1130\n",
      "  Batch 120/188, Loss: 2.2375\n",
      "  Batch 130/188, Loss: 2.6018\n",
      "  Batch 140/188, Loss: 1.8190\n",
      "  Batch 150/188, Loss: 1.8507\n",
      "  Batch 160/188, Loss: 2.2820\n",
      "  Batch 170/188, Loss: 1.8200\n",
      "  Batch 180/188, Loss: 2.3317\n",
      "Epoch 15 Loss: 2.2138\n",
      "  Model saved (improved loss: 2.2138)\n",
      "Epoch 16/100 started\n",
      "  Batch 10/188, Loss: 1.7977\n",
      "  Batch 20/188, Loss: 2.1306\n",
      "  Batch 30/188, Loss: 1.8269\n",
      "  Batch 40/188, Loss: 2.0784\n",
      "  Batch 50/188, Loss: 2.8421\n",
      "  Batch 60/188, Loss: 2.2807\n",
      "  Batch 70/188, Loss: 1.8073\n",
      "  Batch 80/188, Loss: 2.1893\n",
      "  Batch 90/188, Loss: 2.3407\n",
      "  Batch 100/188, Loss: 2.2992\n",
      "  Batch 110/188, Loss: 2.1038\n",
      "  Batch 120/188, Loss: 1.7664\n",
      "  Batch 130/188, Loss: 2.0881\n",
      "  Batch 140/188, Loss: 2.0986\n",
      "  Batch 150/188, Loss: 2.8735\n",
      "  Batch 160/188, Loss: 2.2615\n",
      "  Batch 170/188, Loss: 1.9476\n",
      "  Batch 180/188, Loss: 2.0407\n",
      "Epoch 16 Loss: 2.1714\n",
      "  Model saved (improved loss: 2.1714)\n",
      "Epoch 17/100 started\n",
      "  Batch 10/188, Loss: 1.9582\n",
      "  Batch 20/188, Loss: 2.4185\n",
      "  Batch 30/188, Loss: 2.1228\n",
      "  Batch 40/188, Loss: 2.3324\n",
      "  Batch 50/188, Loss: 1.8798\n",
      "  Batch 60/188, Loss: 1.9109\n",
      "  Batch 70/188, Loss: 2.0993\n",
      "  Batch 80/188, Loss: 1.7967\n",
      "  Batch 90/188, Loss: 2.2616\n",
      "  Batch 100/188, Loss: 2.3028\n",
      "  Batch 110/188, Loss: 1.7964\n",
      "  Batch 120/188, Loss: 2.2514\n",
      "  Batch 130/188, Loss: 2.1503\n",
      "  Batch 140/188, Loss: 1.7765\n",
      "  Batch 150/188, Loss: 1.6333\n",
      "  Batch 160/188, Loss: 2.1624\n",
      "  Batch 170/188, Loss: 2.6320\n",
      "  Batch 180/188, Loss: 2.3624\n",
      "Epoch 17 Loss: 2.1771\n",
      "  No improvement for 1 epochs\n",
      "Epoch 18/100 started\n",
      "  Batch 10/188, Loss: 1.9474\n",
      "  Batch 20/188, Loss: 2.0018\n",
      "  Batch 30/188, Loss: 1.7877\n",
      "  Batch 40/188, Loss: 1.9009\n",
      "  Batch 50/188, Loss: 1.8791\n",
      "  Batch 60/188, Loss: 2.2416\n",
      "  Batch 70/188, Loss: 2.0184\n",
      "  Batch 80/188, Loss: 2.1438\n",
      "  Batch 90/188, Loss: 1.8939\n",
      "  Batch 100/188, Loss: 1.9406\n",
      "  Batch 110/188, Loss: 2.5326\n",
      "  Batch 120/188, Loss: 2.3737\n",
      "  Batch 130/188, Loss: 2.0184\n",
      "  Batch 140/188, Loss: 1.8892\n",
      "  Batch 150/188, Loss: 2.0946\n",
      "  Batch 160/188, Loss: 2.3589\n",
      "  Batch 170/188, Loss: 2.7538\n",
      "  Batch 180/188, Loss: 2.2721\n",
      "Epoch 18 Loss: 2.1310\n",
      "  Model saved (improved loss: 2.1310)\n",
      "Epoch 19/100 started\n",
      "  Batch 10/188, Loss: 2.1428\n",
      "  Batch 20/188, Loss: 2.0092\n",
      "  Batch 30/188, Loss: 2.6839\n",
      "  Batch 40/188, Loss: 2.3678\n",
      "  Batch 50/188, Loss: 2.0894\n",
      "  Batch 60/188, Loss: 1.7536\n",
      "  Batch 70/188, Loss: 2.1961\n",
      "  Batch 80/188, Loss: 2.2075\n",
      "  Batch 90/188, Loss: 2.2778\n",
      "  Batch 100/188, Loss: 2.0581\n",
      "  Batch 110/188, Loss: 2.2412\n",
      "  Batch 120/188, Loss: 2.1204\n",
      "  Batch 130/188, Loss: 2.3731\n",
      "  Batch 140/188, Loss: 2.1320\n",
      "  Batch 150/188, Loss: 2.3499\n",
      "  Batch 160/188, Loss: 2.7880\n",
      "  Batch 170/188, Loss: 1.8823\n",
      "  Batch 180/188, Loss: 2.4518\n",
      "Epoch 19 Loss: 2.1016\n",
      "  Model saved (improved loss: 2.1016)\n",
      "Epoch 20/100 started\n",
      "  Batch 10/188, Loss: 2.0600\n",
      "  Batch 20/188, Loss: 1.8297\n",
      "  Batch 30/188, Loss: 1.7621\n",
      "  Batch 40/188, Loss: 1.7167\n",
      "  Batch 50/188, Loss: 2.1018\n",
      "  Batch 60/188, Loss: 1.8051\n",
      "  Batch 70/188, Loss: 2.1640\n",
      "  Batch 80/188, Loss: 2.2903\n",
      "  Batch 90/188, Loss: 2.2306\n",
      "  Batch 100/188, Loss: 2.5112\n",
      "  Batch 110/188, Loss: 2.2254\n",
      "  Batch 120/188, Loss: 2.2396\n",
      "  Batch 130/188, Loss: 1.8797\n",
      "  Batch 140/188, Loss: 1.7369\n",
      "  Batch 150/188, Loss: 1.6762\n",
      "  Batch 160/188, Loss: 1.8077\n",
      "  Batch 170/188, Loss: 1.8243\n",
      "  Batch 180/188, Loss: 1.7042\n",
      "Epoch 20 Loss: 2.0801\n",
      "  Model saved (improved loss: 2.0801)\n",
      "Epoch 21/100 started\n",
      "  Batch 10/188, Loss: 1.7479\n",
      "  Batch 20/188, Loss: 1.9210\n",
      "  Batch 30/188, Loss: 1.9645\n",
      "  Batch 40/188, Loss: 2.3658\n",
      "  Batch 50/188, Loss: 1.7964\n",
      "  Batch 60/188, Loss: 2.1372\n",
      "  Batch 70/188, Loss: 1.7039\n",
      "  Batch 80/188, Loss: 2.0230\n",
      "  Batch 90/188, Loss: 1.8442\n",
      "  Batch 100/188, Loss: 1.9937\n",
      "  Batch 110/188, Loss: 2.1428\n",
      "  Batch 120/188, Loss: 2.0631\n",
      "  Batch 130/188, Loss: 2.3253\n",
      "  Batch 140/188, Loss: 2.3194\n",
      "  Batch 150/188, Loss: 1.9043\n",
      "  Batch 160/188, Loss: 2.5557\n",
      "  Batch 170/188, Loss: 2.0899\n",
      "  Batch 180/188, Loss: 1.5536\n",
      "Epoch 21 Loss: 2.0551\n",
      "  Model saved (improved loss: 2.0551)\n",
      "Epoch 22/100 started\n",
      "  Batch 10/188, Loss: 1.9826\n",
      "  Batch 20/188, Loss: 1.9940\n",
      "  Batch 30/188, Loss: 2.0688\n",
      "  Batch 40/188, Loss: 1.4086\n",
      "  Batch 50/188, Loss: 2.3984\n",
      "  Batch 60/188, Loss: 2.1280\n",
      "  Batch 70/188, Loss: 1.8241\n",
      "  Batch 80/188, Loss: 2.1089\n",
      "  Batch 90/188, Loss: 2.3156\n",
      "  Batch 100/188, Loss: 1.8004\n",
      "  Batch 110/188, Loss: 1.5475\n",
      "  Batch 120/188, Loss: 2.0542\n",
      "  Batch 130/188, Loss: 1.8691\n",
      "  Batch 140/188, Loss: 1.9371\n",
      "  Batch 150/188, Loss: 1.9828\n",
      "  Batch 160/188, Loss: 1.8115\n",
      "  Batch 170/188, Loss: 1.4884\n",
      "  Batch 180/188, Loss: 2.1817\n",
      "Epoch 22 Loss: 2.0373\n",
      "  Model saved (improved loss: 2.0373)\n",
      "Epoch 23/100 started\n",
      "  Batch 10/188, Loss: 2.4108\n",
      "  Batch 20/188, Loss: 1.9564\n",
      "  Batch 30/188, Loss: 1.8698\n",
      "  Batch 40/188, Loss: 1.9498\n",
      "  Batch 50/188, Loss: 1.9706\n",
      "  Batch 60/188, Loss: 2.3871\n",
      "  Batch 70/188, Loss: 1.7930\n",
      "  Batch 80/188, Loss: 2.4804\n",
      "  Batch 90/188, Loss: 1.5218\n",
      "  Batch 100/188, Loss: 2.5303\n",
      "  Batch 110/188, Loss: 1.9471\n",
      "  Batch 120/188, Loss: 2.0999\n",
      "  Batch 130/188, Loss: 2.0054\n",
      "  Batch 140/188, Loss: 1.9132\n",
      "  Batch 150/188, Loss: 2.0226\n",
      "  Batch 160/188, Loss: 2.0116\n",
      "  Batch 170/188, Loss: 2.2934\n",
      "  Batch 180/188, Loss: 2.1378\n",
      "Epoch 23 Loss: 1.9950\n",
      "  Model saved (improved loss: 1.9950)\n",
      "Epoch 24/100 started\n",
      "  Batch 10/188, Loss: 2.0672\n",
      "  Batch 20/188, Loss: 1.7698\n",
      "  Batch 30/188, Loss: 2.2687\n",
      "  Batch 40/188, Loss: 1.8532\n",
      "  Batch 50/188, Loss: 2.0766\n",
      "  Batch 60/188, Loss: 2.2730\n",
      "  Batch 70/188, Loss: 1.9799\n",
      "  Batch 80/188, Loss: 2.4145\n",
      "  Batch 90/188, Loss: 1.7463\n",
      "  Batch 100/188, Loss: 2.2579\n",
      "  Batch 110/188, Loss: 1.8671\n",
      "  Batch 120/188, Loss: 1.7124\n",
      "  Batch 130/188, Loss: 2.1792\n",
      "  Batch 140/188, Loss: 1.9847\n",
      "  Batch 150/188, Loss: 2.2851\n",
      "  Batch 160/188, Loss: 1.6270\n",
      "  Batch 170/188, Loss: 1.9783\n",
      "  Batch 180/188, Loss: 1.8080\n",
      "Epoch 24 Loss: 2.0217\n",
      "  No improvement for 1 epochs\n",
      "Epoch 25/100 started\n",
      "  Batch 10/188, Loss: 2.1767\n",
      "  Batch 20/188, Loss: 1.5305\n",
      "  Batch 30/188, Loss: 2.2997\n",
      "  Batch 40/188, Loss: 2.3456\n",
      "  Batch 50/188, Loss: 2.0793\n",
      "  Batch 60/188, Loss: 1.7488\n",
      "  Batch 70/188, Loss: 2.0882\n",
      "  Batch 80/188, Loss: 2.1552\n",
      "  Batch 90/188, Loss: 1.7095\n",
      "  Batch 100/188, Loss: 1.8978\n",
      "  Batch 110/188, Loss: 2.0291\n",
      "  Batch 120/188, Loss: 2.1546\n",
      "  Batch 130/188, Loss: 1.5942\n",
      "  Batch 140/188, Loss: 2.0676\n",
      "  Batch 150/188, Loss: 2.1925\n",
      "  Batch 160/188, Loss: 1.4994\n",
      "  Batch 170/188, Loss: 1.8145\n",
      "  Batch 180/188, Loss: 1.8092\n",
      "Epoch 25 Loss: 1.9862\n",
      "  Model saved (improved loss: 1.9862)\n",
      "Epoch 26/100 started\n",
      "  Batch 10/188, Loss: 1.6987\n",
      "  Batch 20/188, Loss: 2.0566\n",
      "  Batch 30/188, Loss: 1.7737\n",
      "  Batch 40/188, Loss: 2.4317\n",
      "  Batch 50/188, Loss: 1.9339\n",
      "  Batch 60/188, Loss: 2.1957\n",
      "  Batch 70/188, Loss: 1.9929\n",
      "  Batch 80/188, Loss: 2.0541\n",
      "  Batch 90/188, Loss: 1.8765\n",
      "  Batch 100/188, Loss: 1.1837\n",
      "  Batch 110/188, Loss: 2.2094\n",
      "  Batch 120/188, Loss: 2.2874\n",
      "  Batch 130/188, Loss: 1.6869\n",
      "  Batch 140/188, Loss: 2.1313\n",
      "  Batch 150/188, Loss: 2.1942\n",
      "  Batch 160/188, Loss: 2.1871\n",
      "  Batch 170/188, Loss: 2.0920\n",
      "  Batch 180/188, Loss: 1.9825\n",
      "Epoch 26 Loss: 1.9694\n",
      "  Model saved (improved loss: 1.9694)\n",
      "Epoch 27/100 started\n",
      "  Batch 10/188, Loss: 1.8230\n",
      "  Batch 20/188, Loss: 1.6763\n",
      "  Batch 30/188, Loss: 1.9372\n",
      "  Batch 40/188, Loss: 2.0167\n",
      "  Batch 50/188, Loss: 1.7944\n",
      "  Batch 60/188, Loss: 1.9635\n",
      "  Batch 70/188, Loss: 1.7594\n",
      "  Batch 80/188, Loss: 1.9625\n",
      "  Batch 90/188, Loss: 2.1025\n",
      "  Batch 100/188, Loss: 2.2550\n",
      "  Batch 110/188, Loss: 2.2262\n",
      "  Batch 120/188, Loss: 1.9488\n",
      "  Batch 130/188, Loss: 2.7072\n",
      "  Batch 140/188, Loss: 1.7743\n",
      "  Batch 150/188, Loss: 1.6342\n",
      "  Batch 160/188, Loss: 1.7870\n",
      "  Batch 170/188, Loss: 1.8704\n",
      "  Batch 180/188, Loss: 1.8742\n",
      "Epoch 27 Loss: 1.9335\n",
      "  Model saved (improved loss: 1.9335)\n",
      "Epoch 28/100 started\n",
      "  Batch 10/188, Loss: 1.9609\n",
      "  Batch 20/188, Loss: 1.8595\n",
      "  Batch 30/188, Loss: 2.3216\n",
      "  Batch 40/188, Loss: 1.8437\n",
      "  Batch 50/188, Loss: 1.9103\n",
      "  Batch 60/188, Loss: 2.0174\n",
      "  Batch 70/188, Loss: 1.7403\n",
      "  Batch 80/188, Loss: 1.6221\n",
      "  Batch 90/188, Loss: 1.6807\n",
      "  Batch 100/188, Loss: 1.9690\n",
      "  Batch 110/188, Loss: 1.6996\n",
      "  Batch 120/188, Loss: 1.9964\n",
      "  Batch 130/188, Loss: 1.6016\n",
      "  Batch 140/188, Loss: 2.0128\n",
      "  Batch 150/188, Loss: 2.1587\n",
      "  Batch 160/188, Loss: 1.9882\n",
      "  Batch 170/188, Loss: 2.1901\n",
      "  Batch 180/188, Loss: 1.3487\n",
      "Epoch 28 Loss: 1.9226\n",
      "  Model saved (improved loss: 1.9226)\n",
      "Epoch 29/100 started\n",
      "  Batch 10/188, Loss: 2.0700\n",
      "  Batch 20/188, Loss: 1.6911\n",
      "  Batch 30/188, Loss: 2.0837\n",
      "  Batch 40/188, Loss: 2.0411\n",
      "  Batch 50/188, Loss: 1.3357\n",
      "  Batch 60/188, Loss: 2.0084\n",
      "  Batch 70/188, Loss: 1.8143\n",
      "  Batch 80/188, Loss: 1.5736\n",
      "  Batch 90/188, Loss: 1.6052\n",
      "  Batch 100/188, Loss: 1.9014\n",
      "  Batch 110/188, Loss: 1.8986\n",
      "  Batch 120/188, Loss: 1.3491\n",
      "  Batch 130/188, Loss: 1.8898\n",
      "  Batch 140/188, Loss: 1.8327\n",
      "  Batch 150/188, Loss: 2.2922\n",
      "  Batch 160/188, Loss: 1.8576\n",
      "  Batch 170/188, Loss: 1.9194\n",
      "  Batch 180/188, Loss: 2.3433\n",
      "Epoch 29 Loss: 1.8908\n",
      "  Model saved (improved loss: 1.8908)\n",
      "Epoch 30/100 started\n",
      "  Batch 10/188, Loss: 2.1158\n",
      "  Batch 20/188, Loss: 1.9596\n",
      "  Batch 30/188, Loss: 2.0220\n",
      "  Batch 40/188, Loss: 1.8716\n",
      "  Batch 50/188, Loss: 2.1367\n",
      "  Batch 60/188, Loss: 1.9824\n",
      "  Batch 70/188, Loss: 1.5867\n",
      "  Batch 80/188, Loss: 1.6515\n",
      "  Batch 90/188, Loss: 1.8615\n",
      "  Batch 100/188, Loss: 2.0037\n",
      "  Batch 110/188, Loss: 2.2940\n",
      "  Batch 120/188, Loss: 1.6327\n",
      "  Batch 130/188, Loss: 2.2536\n",
      "  Batch 140/188, Loss: 1.8522\n",
      "  Batch 150/188, Loss: 1.8606\n",
      "  Batch 160/188, Loss: 1.8276\n",
      "  Batch 170/188, Loss: 2.0354\n",
      "  Batch 180/188, Loss: 2.0745\n",
      "Epoch 30 Loss: 1.8899\n",
      "  Model saved (improved loss: 1.8899)\n",
      "Epoch 31/100 started\n",
      "  Batch 10/188, Loss: 1.7157\n",
      "  Batch 20/188, Loss: 1.7079\n",
      "  Batch 30/188, Loss: 2.1063\n",
      "  Batch 40/188, Loss: 1.6773\n",
      "  Batch 50/188, Loss: 1.5077\n",
      "  Batch 60/188, Loss: 1.5442\n",
      "  Batch 70/188, Loss: 1.3628\n",
      "  Batch 80/188, Loss: 1.8727\n",
      "  Batch 90/188, Loss: 1.6405\n",
      "  Batch 100/188, Loss: 1.9503\n",
      "  Batch 110/188, Loss: 1.7424\n",
      "  Batch 120/188, Loss: 1.6273\n",
      "  Batch 130/188, Loss: 1.8153\n",
      "  Batch 140/188, Loss: 1.4904\n",
      "  Batch 150/188, Loss: 1.7987\n",
      "  Batch 160/188, Loss: 1.9150\n",
      "  Batch 170/188, Loss: 1.9568\n",
      "  Batch 180/188, Loss: 1.6253\n",
      "Epoch 31 Loss: 1.8696\n",
      "  Model saved (improved loss: 1.8696)\n",
      "Epoch 32/100 started\n",
      "  Batch 10/188, Loss: 2.6511\n",
      "  Batch 20/188, Loss: 1.5532\n",
      "  Batch 30/188, Loss: 1.7895\n",
      "  Batch 40/188, Loss: 2.0243\n",
      "  Batch 50/188, Loss: 1.5180\n",
      "  Batch 60/188, Loss: 1.8434\n",
      "  Batch 70/188, Loss: 2.4083\n",
      "  Batch 80/188, Loss: 2.3582\n",
      "  Batch 90/188, Loss: 1.8422\n",
      "  Batch 100/188, Loss: 1.7335\n",
      "  Batch 110/188, Loss: 2.1045\n",
      "  Batch 120/188, Loss: 1.5633\n",
      "  Batch 130/188, Loss: 1.4717\n",
      "  Batch 140/188, Loss: 1.3262\n",
      "  Batch 150/188, Loss: 1.6808\n",
      "  Batch 160/188, Loss: 1.8664\n",
      "  Batch 170/188, Loss: 1.7881\n",
      "  Batch 180/188, Loss: 1.9896\n",
      "Epoch 32 Loss: 1.8516\n",
      "  Model saved (improved loss: 1.8516)\n",
      "Epoch 33/100 started\n",
      "  Batch 10/188, Loss: 2.1124\n",
      "  Batch 20/188, Loss: 1.8392\n",
      "  Batch 30/188, Loss: 1.6921\n",
      "  Batch 40/188, Loss: 1.9659\n",
      "  Batch 50/188, Loss: 1.9608\n",
      "  Batch 60/188, Loss: 1.7708\n",
      "  Batch 70/188, Loss: 2.2623\n",
      "  Batch 80/188, Loss: 1.7886\n",
      "  Batch 90/188, Loss: 1.8446\n",
      "  Batch 100/188, Loss: 2.4131\n",
      "  Batch 110/188, Loss: 2.0624\n",
      "  Batch 120/188, Loss: 1.7390\n",
      "  Batch 130/188, Loss: 2.2786\n",
      "  Batch 140/188, Loss: 1.4640\n",
      "  Batch 150/188, Loss: 1.9966\n",
      "  Batch 160/188, Loss: 2.0596\n",
      "  Batch 170/188, Loss: 1.7727\n",
      "  Batch 180/188, Loss: 1.6465\n",
      "Epoch 33 Loss: 1.8178\n",
      "  Model saved (improved loss: 1.8178)\n",
      "Epoch 34/100 started\n",
      "  Batch 10/188, Loss: 1.5319\n",
      "  Batch 20/188, Loss: 1.6541\n",
      "  Batch 30/188, Loss: 2.0858\n",
      "  Batch 40/188, Loss: 1.9664\n",
      "  Batch 50/188, Loss: 1.5677\n",
      "  Batch 60/188, Loss: 1.4306\n",
      "  Batch 70/188, Loss: 1.8742\n",
      "  Batch 80/188, Loss: 1.8061\n",
      "  Batch 90/188, Loss: 1.7548\n",
      "  Batch 100/188, Loss: 2.1017\n",
      "  Batch 110/188, Loss: 1.7530\n",
      "  Batch 120/188, Loss: 1.5663\n",
      "  Batch 130/188, Loss: 1.6320\n",
      "  Batch 140/188, Loss: 1.9381\n",
      "  Batch 150/188, Loss: 2.2983\n",
      "  Batch 160/188, Loss: 1.7035\n",
      "  Batch 170/188, Loss: 2.0010\n",
      "  Batch 180/188, Loss: 1.3204\n",
      "Epoch 34 Loss: 1.8181\n",
      "  No improvement for 1 epochs\n",
      "Epoch 35/100 started\n",
      "  Batch 10/188, Loss: 1.4208\n",
      "  Batch 20/188, Loss: 1.2167\n",
      "  Batch 30/188, Loss: 2.1610\n",
      "  Batch 40/188, Loss: 1.9493\n",
      "  Batch 50/188, Loss: 1.7207\n",
      "  Batch 60/188, Loss: 1.7344\n",
      "  Batch 70/188, Loss: 1.7651\n",
      "  Batch 80/188, Loss: 1.6535\n",
      "  Batch 90/188, Loss: 2.1691\n",
      "  Batch 100/188, Loss: 2.2830\n",
      "  Batch 110/188, Loss: 2.3485\n",
      "  Batch 120/188, Loss: 1.8075\n",
      "  Batch 130/188, Loss: 3.1634\n",
      "  Batch 140/188, Loss: 1.4168\n",
      "  Batch 150/188, Loss: 1.8141\n",
      "  Batch 160/188, Loss: 1.7209\n",
      "  Batch 170/188, Loss: 1.6969\n",
      "  Batch 180/188, Loss: 1.9006\n",
      "Epoch 35 Loss: 1.7962\n",
      "  Model saved (improved loss: 1.7962)\n",
      "Epoch 36/100 started\n",
      "  Batch 10/188, Loss: 1.6649\n",
      "  Batch 20/188, Loss: 1.8420\n",
      "  Batch 30/188, Loss: 1.6666\n",
      "  Batch 40/188, Loss: 1.5277\n",
      "  Batch 50/188, Loss: 1.7144\n",
      "  Batch 60/188, Loss: 1.4975\n",
      "  Batch 70/188, Loss: 1.5862\n",
      "  Batch 80/188, Loss: 1.9671\n",
      "  Batch 90/188, Loss: 1.9883\n",
      "  Batch 100/188, Loss: 1.9052\n",
      "  Batch 110/188, Loss: 2.0187\n",
      "  Batch 120/188, Loss: 1.7479\n",
      "  Batch 130/188, Loss: 1.6299\n",
      "  Batch 140/188, Loss: 1.5907\n",
      "  Batch 150/188, Loss: 1.7683\n",
      "  Batch 160/188, Loss: 1.7131\n",
      "  Batch 170/188, Loss: 1.5403\n",
      "  Batch 180/188, Loss: 1.4641\n",
      "Epoch 36 Loss: 1.7762\n",
      "  Model saved (improved loss: 1.7762)\n",
      "Epoch 37/100 started\n",
      "  Batch 10/188, Loss: 2.0456\n",
      "  Batch 20/188, Loss: 1.9244\n",
      "  Batch 30/188, Loss: 1.7142\n",
      "  Batch 40/188, Loss: 1.6576\n",
      "  Batch 50/188, Loss: 2.2621\n",
      "  Batch 60/188, Loss: 1.9546\n",
      "  Batch 70/188, Loss: 1.5401\n",
      "  Batch 80/188, Loss: 1.7202\n",
      "  Batch 90/188, Loss: 1.7065\n",
      "  Batch 100/188, Loss: 1.5967\n",
      "  Batch 110/188, Loss: 1.8126\n",
      "  Batch 120/188, Loss: 2.0077\n",
      "  Batch 130/188, Loss: 1.7346\n",
      "  Batch 140/188, Loss: 1.7781\n",
      "  Batch 150/188, Loss: 2.0548\n",
      "  Batch 160/188, Loss: 2.1786\n",
      "  Batch 170/188, Loss: 1.6531\n",
      "  Batch 180/188, Loss: 1.5454\n",
      "Epoch 37 Loss: 1.7614\n",
      "  Model saved (improved loss: 1.7614)\n",
      "Epoch 38/100 started\n",
      "  Batch 10/188, Loss: 2.2143\n",
      "  Batch 20/188, Loss: 1.7865\n",
      "  Batch 30/188, Loss: 1.5134\n",
      "  Batch 40/188, Loss: 1.2825\n",
      "  Batch 50/188, Loss: 2.0964\n",
      "  Batch 60/188, Loss: 1.9183\n",
      "  Batch 70/188, Loss: 1.9275\n",
      "  Batch 80/188, Loss: 1.4457\n",
      "  Batch 90/188, Loss: 1.9340\n",
      "  Batch 100/188, Loss: 1.6648\n",
      "  Batch 110/188, Loss: 1.7987\n",
      "  Batch 120/188, Loss: 1.4270\n",
      "  Batch 130/188, Loss: 1.7401\n",
      "  Batch 140/188, Loss: 1.6001\n",
      "  Batch 150/188, Loss: 1.2349\n",
      "  Batch 160/188, Loss: 1.6092\n",
      "  Batch 170/188, Loss: 2.1384\n",
      "  Batch 180/188, Loss: 1.5514\n",
      "Epoch 38 Loss: 1.7573\n",
      "  Model saved (improved loss: 1.7573)\n",
      "Epoch 39/100 started\n",
      "  Batch 10/188, Loss: 1.7444\n",
      "  Batch 20/188, Loss: 1.8101\n",
      "  Batch 30/188, Loss: 1.4098\n",
      "  Batch 40/188, Loss: 1.6524\n",
      "  Batch 50/188, Loss: 2.5830\n",
      "  Batch 60/188, Loss: 1.6395\n",
      "  Batch 70/188, Loss: 1.6287\n",
      "  Batch 80/188, Loss: 1.6999\n",
      "  Batch 90/188, Loss: 1.6095\n",
      "  Batch 100/188, Loss: 1.7646\n",
      "  Batch 110/188, Loss: 1.7363\n",
      "  Batch 120/188, Loss: 1.7642\n",
      "  Batch 130/188, Loss: 1.8806\n",
      "  Batch 140/188, Loss: 1.2607\n",
      "  Batch 150/188, Loss: 1.9500\n",
      "  Batch 160/188, Loss: 1.8505\n",
      "  Batch 170/188, Loss: 2.0801\n",
      "  Batch 180/188, Loss: 1.9655\n",
      "Epoch 39 Loss: 1.7557\n",
      "  Model saved (improved loss: 1.7557)\n",
      "Epoch 40/100 started\n",
      "  Batch 10/188, Loss: 1.5479\n",
      "  Batch 20/188, Loss: 1.7292\n",
      "  Batch 30/188, Loss: 1.7926\n",
      "  Batch 40/188, Loss: 1.6241\n",
      "  Batch 50/188, Loss: 1.8213\n",
      "  Batch 60/188, Loss: 1.4847\n",
      "  Batch 70/188, Loss: 1.8946\n",
      "  Batch 80/188, Loss: 1.6055\n",
      "  Batch 90/188, Loss: 1.5710\n",
      "  Batch 100/188, Loss: 1.2951\n",
      "  Batch 110/188, Loss: 1.2824\n",
      "  Batch 120/188, Loss: 1.4597\n",
      "  Batch 130/188, Loss: 1.7286\n",
      "  Batch 140/188, Loss: 1.8843\n",
      "  Batch 150/188, Loss: 1.4233\n",
      "  Batch 160/188, Loss: 2.2127\n",
      "  Batch 170/188, Loss: 1.9108\n",
      "  Batch 180/188, Loss: 1.9438\n",
      "Epoch 40 Loss: 1.7128\n",
      "  Model saved (improved loss: 1.7128)\n",
      "Epoch 41/100 started\n",
      "  Batch 10/188, Loss: 2.1701\n",
      "  Batch 20/188, Loss: 1.5665\n",
      "  Batch 30/188, Loss: 1.5086\n",
      "  Batch 40/188, Loss: 1.8550\n",
      "  Batch 50/188, Loss: 2.1033\n",
      "  Batch 60/188, Loss: 1.7382\n",
      "  Batch 70/188, Loss: 1.8546\n",
      "  Batch 80/188, Loss: 1.6258\n",
      "  Batch 90/188, Loss: 1.8634\n",
      "  Batch 100/188, Loss: 1.9210\n",
      "  Batch 110/188, Loss: 2.0358\n",
      "  Batch 120/188, Loss: 1.6285\n",
      "  Batch 130/188, Loss: 1.7803\n",
      "  Batch 140/188, Loss: 1.7814\n",
      "  Batch 150/188, Loss: 1.3460\n",
      "  Batch 160/188, Loss: 1.6818\n",
      "  Batch 170/188, Loss: 2.1408\n",
      "  Batch 180/188, Loss: 1.9097\n",
      "Epoch 41 Loss: 1.7003\n",
      "  Model saved (improved loss: 1.7003)\n",
      "Epoch 42/100 started\n",
      "  Batch 10/188, Loss: 1.7684\n",
      "  Batch 20/188, Loss: 1.0412\n",
      "  Batch 30/188, Loss: 1.7309\n",
      "  Batch 40/188, Loss: 1.7046\n",
      "  Batch 50/188, Loss: 1.7617\n",
      "  Batch 60/188, Loss: 1.4726\n",
      "  Batch 70/188, Loss: 1.9430\n",
      "  Batch 80/188, Loss: 2.1125\n",
      "  Batch 90/188, Loss: 1.4803\n",
      "  Batch 100/188, Loss: 1.5850\n",
      "  Batch 110/188, Loss: 1.7552\n",
      "  Batch 120/188, Loss: 1.6442\n",
      "  Batch 130/188, Loss: 1.5775\n",
      "  Batch 140/188, Loss: 1.9721\n",
      "  Batch 150/188, Loss: 2.1816\n",
      "  Batch 160/188, Loss: 1.3425\n",
      "  Batch 170/188, Loss: 1.6016\n",
      "  Batch 180/188, Loss: 2.4101\n",
      "Epoch 42 Loss: 1.7074\n",
      "  No improvement for 1 epochs\n",
      "Epoch 43/100 started\n",
      "  Batch 10/188, Loss: 1.7553\n",
      "  Batch 20/188, Loss: 1.8502\n",
      "  Batch 30/188, Loss: 1.1185\n",
      "  Batch 40/188, Loss: 1.7221\n",
      "  Batch 50/188, Loss: 1.4143\n",
      "  Batch 60/188, Loss: 1.3287\n",
      "  Batch 70/188, Loss: 2.4265\n",
      "  Batch 80/188, Loss: 1.6247\n",
      "  Batch 90/188, Loss: 2.2905\n",
      "  Batch 100/188, Loss: 1.9537\n",
      "  Batch 110/188, Loss: 1.4830\n",
      "  Batch 120/188, Loss: 1.6415\n",
      "  Batch 130/188, Loss: 1.6807\n",
      "  Batch 140/188, Loss: 1.8887\n",
      "  Batch 150/188, Loss: 1.8615\n",
      "  Batch 160/188, Loss: 1.9291\n",
      "  Batch 170/188, Loss: 1.3625\n",
      "  Batch 180/188, Loss: 1.5045\n",
      "Epoch 43 Loss: 1.6799\n",
      "  Model saved (improved loss: 1.6799)\n",
      "Epoch 44/100 started\n",
      "  Batch 10/188, Loss: 1.2119\n",
      "  Batch 20/188, Loss: 1.7705\n",
      "  Batch 30/188, Loss: 1.8112\n",
      "  Batch 40/188, Loss: 1.6880\n",
      "  Batch 50/188, Loss: 1.3162\n",
      "  Batch 60/188, Loss: 1.5878\n",
      "  Batch 70/188, Loss: 1.3386\n",
      "  Batch 80/188, Loss: 1.4972\n",
      "  Batch 90/188, Loss: 1.2520\n",
      "  Batch 100/188, Loss: 1.6062\n",
      "  Batch 110/188, Loss: 1.6776\n",
      "  Batch 120/188, Loss: 1.7868\n",
      "  Batch 130/188, Loss: 1.1339\n",
      "  Batch 140/188, Loss: 1.7421\n",
      "  Batch 150/188, Loss: 1.3830\n",
      "  Batch 160/188, Loss: 2.1238\n",
      "  Batch 170/188, Loss: 1.6749\n",
      "  Batch 180/188, Loss: 1.7531\n",
      "Epoch 44 Loss: 1.6699\n",
      "  Model saved (improved loss: 1.6699)\n",
      "Epoch 45/100 started\n",
      "  Batch 10/188, Loss: 1.9147\n",
      "  Batch 20/188, Loss: 1.7398\n",
      "  Batch 30/188, Loss: 1.7610\n",
      "  Batch 40/188, Loss: 1.7949\n",
      "  Batch 50/188, Loss: 1.4313\n",
      "  Batch 60/188, Loss: 1.7036\n",
      "  Batch 70/188, Loss: 1.4522\n",
      "  Batch 80/188, Loss: 1.4893\n",
      "  Batch 90/188, Loss: 1.6863\n",
      "  Batch 100/188, Loss: 1.6584\n",
      "  Batch 110/188, Loss: 1.8294\n",
      "  Batch 120/188, Loss: 1.5751\n",
      "  Batch 130/188, Loss: 1.6454\n",
      "  Batch 140/188, Loss: 1.8194\n",
      "  Batch 150/188, Loss: 1.7059\n",
      "  Batch 160/188, Loss: 1.7042\n",
      "  Batch 170/188, Loss: 1.9150\n",
      "  Batch 180/188, Loss: 1.7273\n",
      "Epoch 45 Loss: 1.6613\n",
      "  Model saved (improved loss: 1.6613)\n",
      "Epoch 46/100 started\n",
      "  Batch 10/188, Loss: 2.0338\n",
      "  Batch 20/188, Loss: 1.4608\n",
      "  Batch 30/188, Loss: 1.6308\n",
      "  Batch 40/188, Loss: 1.8059\n",
      "  Batch 50/188, Loss: 1.4538\n",
      "  Batch 60/188, Loss: 1.7399\n",
      "  Batch 70/188, Loss: 1.5796\n",
      "  Batch 80/188, Loss: 1.7426\n",
      "  Batch 90/188, Loss: 1.6349\n",
      "  Batch 100/188, Loss: 1.7829\n",
      "  Batch 110/188, Loss: 1.3903\n",
      "  Batch 120/188, Loss: 1.4927\n",
      "  Batch 130/188, Loss: 1.5400\n",
      "  Batch 140/188, Loss: 1.0717\n",
      "  Batch 150/188, Loss: 1.4639\n",
      "  Batch 160/188, Loss: 1.7021\n",
      "  Batch 170/188, Loss: 1.4188\n",
      "  Batch 180/188, Loss: 1.0583\n",
      "Epoch 46 Loss: 1.6276\n",
      "  Model saved (improved loss: 1.6276)\n",
      "Epoch 47/100 started\n",
      "  Batch 10/188, Loss: 1.9839\n",
      "  Batch 20/188, Loss: 1.5244\n",
      "  Batch 30/188, Loss: 1.6587\n",
      "  Batch 40/188, Loss: 1.3993\n",
      "  Batch 50/188, Loss: 2.1966\n",
      "  Batch 60/188, Loss: 1.0689\n",
      "  Batch 70/188, Loss: 2.2646\n",
      "  Batch 80/188, Loss: 1.6494\n",
      "  Batch 90/188, Loss: 1.4068\n",
      "  Batch 100/188, Loss: 1.7004\n",
      "  Batch 110/188, Loss: 1.6701\n",
      "  Batch 120/188, Loss: 1.8246\n",
      "  Batch 130/188, Loss: 1.9228\n",
      "  Batch 140/188, Loss: 1.5567\n",
      "  Batch 150/188, Loss: 1.5724\n",
      "  Batch 160/188, Loss: 1.8471\n",
      "  Batch 170/188, Loss: 1.6652\n",
      "  Batch 180/188, Loss: 1.5429\n",
      "Epoch 47 Loss: 1.6249\n",
      "  Model saved (improved loss: 1.6249)\n",
      "Epoch 48/100 started\n",
      "  Batch 10/188, Loss: 1.4729\n",
      "  Batch 20/188, Loss: 2.0417\n",
      "  Batch 30/188, Loss: 1.7517\n",
      "  Batch 40/188, Loss: 1.6295\n",
      "  Batch 50/188, Loss: 1.8075\n",
      "  Batch 60/188, Loss: 1.5650\n",
      "  Batch 70/188, Loss: 1.2876\n",
      "  Batch 80/188, Loss: 2.1929\n",
      "  Batch 90/188, Loss: 1.6644\n",
      "  Batch 100/188, Loss: 2.1797\n",
      "  Batch 110/188, Loss: 2.0081\n",
      "  Batch 120/188, Loss: 1.3873\n",
      "  Batch 130/188, Loss: 1.7677\n",
      "  Batch 140/188, Loss: 1.6657\n",
      "  Batch 150/188, Loss: 1.6242\n",
      "  Batch 160/188, Loss: 1.8568\n",
      "  Batch 170/188, Loss: 1.6583\n",
      "  Batch 180/188, Loss: 1.5559\n",
      "Epoch 48 Loss: 1.6095\n",
      "  Model saved (improved loss: 1.6095)\n",
      "Epoch 49/100 started\n",
      "  Batch 10/188, Loss: 1.1980\n",
      "  Batch 20/188, Loss: 2.2880\n",
      "  Batch 30/188, Loss: 1.4264\n",
      "  Batch 40/188, Loss: 1.2287\n",
      "  Batch 50/188, Loss: 1.1115\n",
      "  Batch 60/188, Loss: 1.5808\n",
      "  Batch 70/188, Loss: 1.7774\n",
      "  Batch 80/188, Loss: 1.7514\n",
      "  Batch 90/188, Loss: 1.6019\n",
      "  Batch 100/188, Loss: 1.9720\n",
      "  Batch 110/188, Loss: 1.6039\n",
      "  Batch 120/188, Loss: 1.6501\n",
      "  Batch 130/188, Loss: 1.1984\n",
      "  Batch 140/188, Loss: 1.4224\n",
      "  Batch 150/188, Loss: 1.1209\n",
      "  Batch 160/188, Loss: 1.8832\n",
      "  Batch 170/188, Loss: 2.3644\n",
      "  Batch 180/188, Loss: 1.6710\n",
      "Epoch 49 Loss: 1.6023\n",
      "  Model saved (improved loss: 1.6023)\n",
      "Epoch 50/100 started\n",
      "  Batch 10/188, Loss: 1.7237\n",
      "  Batch 20/188, Loss: 1.1183\n",
      "  Batch 30/188, Loss: 1.5312\n",
      "  Batch 40/188, Loss: 1.8035\n",
      "  Batch 50/188, Loss: 1.4525\n",
      "  Batch 60/188, Loss: 1.4005\n",
      "  Batch 70/188, Loss: 1.6036\n",
      "  Batch 80/188, Loss: 1.9047\n",
      "  Batch 90/188, Loss: 1.7195\n",
      "  Batch 100/188, Loss: 1.5619\n",
      "  Batch 110/188, Loss: 1.8429\n",
      "  Batch 120/188, Loss: 1.7306\n",
      "  Batch 130/188, Loss: 1.2246\n",
      "  Batch 140/188, Loss: 1.6212\n",
      "  Batch 150/188, Loss: 1.5326\n",
      "  Batch 160/188, Loss: 1.8058\n",
      "  Batch 170/188, Loss: 1.4799\n",
      "  Batch 180/188, Loss: 1.5474\n",
      "Epoch 50 Loss: 1.6152\n",
      "  No improvement for 1 epochs\n",
      "Epoch 51/100 started\n",
      "  Batch 10/188, Loss: 1.2862\n",
      "  Batch 20/188, Loss: 1.5680\n",
      "  Batch 30/188, Loss: 1.1644\n",
      "  Batch 40/188, Loss: 1.4047\n",
      "  Batch 50/188, Loss: 1.7092\n",
      "  Batch 60/188, Loss: 1.2713\n",
      "  Batch 70/188, Loss: 1.7903\n",
      "  Batch 80/188, Loss: 1.9782\n",
      "  Batch 90/188, Loss: 1.4790\n",
      "  Batch 100/188, Loss: 2.2518\n",
      "  Batch 110/188, Loss: 1.1162\n",
      "  Batch 120/188, Loss: 0.8923\n",
      "  Batch 130/188, Loss: 2.3584\n",
      "  Batch 140/188, Loss: 1.4447\n",
      "  Batch 150/188, Loss: 1.5276\n",
      "  Batch 160/188, Loss: 1.3609\n",
      "  Batch 170/188, Loss: 1.7716\n",
      "  Batch 180/188, Loss: 1.9477\n",
      "Epoch 51 Loss: 1.5726\n",
      "  Model saved (improved loss: 1.5726)\n",
      "Epoch 52/100 started\n",
      "  Batch 10/188, Loss: 1.5458\n",
      "  Batch 20/188, Loss: 1.6760\n",
      "  Batch 30/188, Loss: 1.4816\n",
      "  Batch 40/188, Loss: 1.8528\n",
      "  Batch 50/188, Loss: 1.2399\n",
      "  Batch 60/188, Loss: 1.7518\n",
      "  Batch 70/188, Loss: 1.6024\n",
      "  Batch 80/188, Loss: 1.6423\n",
      "  Batch 90/188, Loss: 2.1654\n",
      "  Batch 100/188, Loss: 1.7769\n",
      "  Batch 110/188, Loss: 1.5454\n",
      "  Batch 120/188, Loss: 1.4097\n",
      "  Batch 130/188, Loss: 2.2442\n",
      "  Batch 140/188, Loss: 1.2933\n",
      "  Batch 150/188, Loss: 1.7881\n",
      "  Batch 160/188, Loss: 1.7913\n",
      "  Batch 170/188, Loss: 1.4906\n",
      "  Batch 180/188, Loss: 1.6682\n",
      "Epoch 52 Loss: 1.5732\n",
      "  No improvement for 1 epochs\n",
      "Epoch 53/100 started\n",
      "  Batch 10/188, Loss: 1.8219\n",
      "  Batch 20/188, Loss: 1.5110\n",
      "  Batch 30/188, Loss: 1.7016\n",
      "  Batch 40/188, Loss: 1.4679\n",
      "  Batch 50/188, Loss: 1.8535\n",
      "  Batch 60/188, Loss: 1.5694\n",
      "  Batch 70/188, Loss: 1.4624\n",
      "  Batch 80/188, Loss: 1.6497\n",
      "  Batch 90/188, Loss: 1.6507\n",
      "  Batch 100/188, Loss: 1.8737\n",
      "  Batch 110/188, Loss: 1.6206\n",
      "  Batch 120/188, Loss: 1.8915\n",
      "  Batch 130/188, Loss: 1.3972\n",
      "  Batch 140/188, Loss: 1.4491\n",
      "  Batch 150/188, Loss: 1.6062\n",
      "  Batch 160/188, Loss: 1.4632\n",
      "  Batch 170/188, Loss: 1.4651\n",
      "  Batch 180/188, Loss: 1.4393\n",
      "Epoch 53 Loss: 1.5584\n",
      "  Model saved (improved loss: 1.5584)\n",
      "Epoch 54/100 started\n",
      "  Batch 10/188, Loss: 1.1585\n",
      "  Batch 20/188, Loss: 1.4864\n",
      "  Batch 30/188, Loss: 1.2788\n",
      "  Batch 40/188, Loss: 1.7660\n",
      "  Batch 50/188, Loss: 1.7347\n",
      "  Batch 60/188, Loss: 2.1269\n",
      "  Batch 70/188, Loss: 1.4570\n",
      "  Batch 80/188, Loss: 1.2817\n",
      "  Batch 90/188, Loss: 2.0046\n",
      "  Batch 100/188, Loss: 1.6516\n",
      "  Batch 110/188, Loss: 1.4784\n",
      "  Batch 120/188, Loss: 1.5875\n",
      "  Batch 130/188, Loss: 2.1543\n",
      "  Batch 140/188, Loss: 1.5640\n",
      "  Batch 150/188, Loss: 1.2084\n",
      "  Batch 160/188, Loss: 1.4770\n",
      "  Batch 170/188, Loss: 1.5153\n",
      "  Batch 180/188, Loss: 1.3514\n",
      "Epoch 54 Loss: 1.5352\n",
      "  Model saved (improved loss: 1.5352)\n",
      "Epoch 55/100 started\n",
      "  Batch 10/188, Loss: 1.3691\n",
      "  Batch 20/188, Loss: 1.4039\n",
      "  Batch 30/188, Loss: 1.6758\n",
      "  Batch 40/188, Loss: 1.3060\n",
      "  Batch 50/188, Loss: 1.4772\n",
      "  Batch 60/188, Loss: 1.2714\n",
      "  Batch 70/188, Loss: 1.6853\n",
      "  Batch 80/188, Loss: 1.6029\n",
      "  Batch 90/188, Loss: 1.4761\n",
      "  Batch 100/188, Loss: 1.8537\n",
      "  Batch 110/188, Loss: 1.7635\n",
      "  Batch 120/188, Loss: 1.7767\n",
      "  Batch 130/188, Loss: 0.9811\n",
      "  Batch 140/188, Loss: 1.1478\n",
      "  Batch 150/188, Loss: 1.4148\n",
      "  Batch 160/188, Loss: 1.4385\n",
      "  Batch 170/188, Loss: 1.5444\n",
      "  Batch 180/188, Loss: 2.3774\n",
      "Epoch 55 Loss: 1.5470\n",
      "  No improvement for 1 epochs\n",
      "Epoch 56/100 started\n",
      "  Batch 10/188, Loss: 1.2646\n",
      "  Batch 20/188, Loss: 1.5685\n",
      "  Batch 30/188, Loss: 1.1348\n",
      "  Batch 40/188, Loss: 1.1539\n",
      "  Batch 50/188, Loss: 1.6673\n",
      "  Batch 60/188, Loss: 1.4897\n",
      "  Batch 70/188, Loss: 1.7840\n",
      "  Batch 80/188, Loss: 1.5516\n",
      "  Batch 90/188, Loss: 1.0905\n",
      "  Batch 100/188, Loss: 2.0032\n",
      "  Batch 110/188, Loss: 1.3896\n",
      "  Batch 120/188, Loss: 1.0894\n",
      "  Batch 130/188, Loss: 1.5339\n",
      "  Batch 140/188, Loss: 1.5239\n",
      "  Batch 150/188, Loss: 2.0620\n",
      "  Batch 160/188, Loss: 1.7261\n",
      "  Batch 170/188, Loss: 2.1462\n",
      "  Batch 180/188, Loss: 1.9855\n",
      "Epoch 56 Loss: 1.5467\n",
      "  No improvement for 2 epochs\n",
      "Epoch 57/100 started\n",
      "  Batch 10/188, Loss: 1.5398\n",
      "  Batch 20/188, Loss: 1.6053\n",
      "  Batch 30/188, Loss: 1.0613\n",
      "  Batch 40/188, Loss: 1.7030\n",
      "  Batch 50/188, Loss: 1.6128\n",
      "  Batch 60/188, Loss: 1.8089\n",
      "  Batch 70/188, Loss: 1.5285\n",
      "  Batch 80/188, Loss: 1.0361\n",
      "  Batch 90/188, Loss: 1.0133\n",
      "  Batch 100/188, Loss: 1.3052\n",
      "  Batch 110/188, Loss: 1.2273\n",
      "  Batch 120/188, Loss: 1.6114\n",
      "  Batch 130/188, Loss: 1.4234\n",
      "  Batch 140/188, Loss: 1.7379\n",
      "  Batch 150/188, Loss: 1.3316\n",
      "  Batch 160/188, Loss: 1.5067\n",
      "  Batch 170/188, Loss: 1.4752\n",
      "  Batch 180/188, Loss: 1.1769\n",
      "Epoch 57 Loss: 1.5209\n",
      "  Model saved (improved loss: 1.5209)\n",
      "Epoch 58/100 started\n",
      "  Batch 10/188, Loss: 1.7151\n",
      "  Batch 20/188, Loss: 1.5855\n",
      "  Batch 30/188, Loss: 1.8593\n",
      "  Batch 40/188, Loss: 1.9483\n",
      "  Batch 50/188, Loss: 1.4382\n",
      "  Batch 60/188, Loss: 1.0088\n",
      "  Batch 70/188, Loss: 1.7027\n",
      "  Batch 80/188, Loss: 1.7311\n",
      "  Batch 90/188, Loss: 1.5469\n",
      "  Batch 100/188, Loss: 1.2402\n",
      "  Batch 110/188, Loss: 1.5519\n",
      "  Batch 120/188, Loss: 1.6720\n",
      "  Batch 130/188, Loss: 1.2331\n",
      "  Batch 140/188, Loss: 1.2156\n",
      "  Batch 150/188, Loss: 1.3528\n",
      "  Batch 160/188, Loss: 1.2935\n",
      "  Batch 170/188, Loss: 1.1923\n",
      "  Batch 180/188, Loss: 1.5654\n",
      "Epoch 58 Loss: 1.5205\n",
      "  Model saved (improved loss: 1.5205)\n",
      "Epoch 59/100 started\n",
      "  Batch 10/188, Loss: 1.1774\n",
      "  Batch 20/188, Loss: 1.5347\n",
      "  Batch 30/188, Loss: 1.2694\n",
      "  Batch 40/188, Loss: 1.6704\n",
      "  Batch 50/188, Loss: 1.1381\n",
      "  Batch 60/188, Loss: 1.5567\n",
      "  Batch 70/188, Loss: 1.2665\n",
      "  Batch 80/188, Loss: 2.0078\n",
      "  Batch 90/188, Loss: 1.2133\n",
      "  Batch 100/188, Loss: 1.5078\n",
      "  Batch 110/188, Loss: 1.0948\n",
      "  Batch 120/188, Loss: 1.7308\n",
      "  Batch 130/188, Loss: 1.7493\n",
      "  Batch 140/188, Loss: 1.3270\n",
      "  Batch 150/188, Loss: 1.8442\n",
      "  Batch 160/188, Loss: 1.7645\n",
      "  Batch 170/188, Loss: 1.1993\n",
      "  Batch 180/188, Loss: 1.2725\n",
      "Epoch 59 Loss: 1.5339\n",
      "  No improvement for 1 epochs\n",
      "Epoch 60/100 started\n",
      "  Batch 10/188, Loss: 1.3835\n",
      "  Batch 20/188, Loss: 1.4688\n",
      "  Batch 30/188, Loss: 1.7444\n",
      "  Batch 40/188, Loss: 1.7038\n",
      "  Batch 50/188, Loss: 1.8065\n",
      "  Batch 60/188, Loss: 1.1274\n",
      "  Batch 70/188, Loss: 1.6755\n",
      "  Batch 80/188, Loss: 1.7960\n",
      "  Batch 90/188, Loss: 1.5735\n",
      "  Batch 100/188, Loss: 1.2521\n",
      "  Batch 110/188, Loss: 1.6692\n",
      "  Batch 120/188, Loss: 1.4395\n",
      "  Batch 130/188, Loss: 1.1522\n",
      "  Batch 140/188, Loss: 1.1112\n",
      "  Batch 150/188, Loss: 0.8384\n",
      "  Batch 160/188, Loss: 1.1628\n",
      "  Batch 170/188, Loss: 1.1694\n",
      "  Batch 180/188, Loss: 1.3106\n",
      "Epoch 60 Loss: 1.4912\n",
      "  Model saved (improved loss: 1.4912)\n",
      "Epoch 61/100 started\n",
      "  Batch 10/188, Loss: 1.7402\n",
      "  Batch 20/188, Loss: 1.9745\n",
      "  Batch 30/188, Loss: 1.4587\n",
      "  Batch 40/188, Loss: 1.8044\n",
      "  Batch 50/188, Loss: 1.6847\n",
      "  Batch 60/188, Loss: 1.5800\n",
      "  Batch 70/188, Loss: 1.3575\n",
      "  Batch 80/188, Loss: 1.3143\n",
      "  Batch 90/188, Loss: 1.6859\n",
      "  Batch 100/188, Loss: 1.3410\n",
      "  Batch 110/188, Loss: 1.4670\n",
      "  Batch 120/188, Loss: 1.2766\n",
      "  Batch 130/188, Loss: 1.5612\n",
      "  Batch 140/188, Loss: 0.9730\n",
      "  Batch 150/188, Loss: 1.6010\n",
      "  Batch 160/188, Loss: 0.8446\n",
      "  Batch 170/188, Loss: 1.1900\n",
      "  Batch 180/188, Loss: 0.9354\n",
      "Epoch 61 Loss: 1.4805\n",
      "  Model saved (improved loss: 1.4805)\n",
      "Epoch 62/100 started\n",
      "  Batch 10/188, Loss: 1.6875\n",
      "  Batch 20/188, Loss: 1.1424\n",
      "  Batch 30/188, Loss: 1.7890\n",
      "  Batch 40/188, Loss: 1.2527\n",
      "  Batch 50/188, Loss: 1.3158\n",
      "  Batch 60/188, Loss: 1.9693\n",
      "  Batch 70/188, Loss: 2.0312\n",
      "  Batch 80/188, Loss: 1.2149\n",
      "  Batch 90/188, Loss: 1.5464\n",
      "  Batch 100/188, Loss: 1.4051\n",
      "  Batch 110/188, Loss: 1.7528\n",
      "  Batch 120/188, Loss: 1.5974\n",
      "  Batch 130/188, Loss: 1.7496\n",
      "  Batch 140/188, Loss: 1.5965\n",
      "  Batch 150/188, Loss: 1.2873\n",
      "  Batch 160/188, Loss: 1.1327\n",
      "  Batch 170/188, Loss: 1.7030\n",
      "  Batch 180/188, Loss: 1.7218\n",
      "Epoch 62 Loss: 1.4702\n",
      "  Model saved (improved loss: 1.4702)\n",
      "Epoch 63/100 started\n",
      "  Batch 10/188, Loss: 1.6474\n",
      "  Batch 20/188, Loss: 1.2850\n",
      "  Batch 30/188, Loss: 1.6934\n",
      "  Batch 40/188, Loss: 1.5691\n",
      "  Batch 50/188, Loss: 1.5711\n",
      "  Batch 60/188, Loss: 1.2382\n",
      "  Batch 70/188, Loss: 1.2918\n",
      "  Batch 80/188, Loss: 1.3813\n",
      "  Batch 90/188, Loss: 1.5395\n",
      "  Batch 100/188, Loss: 1.6097\n",
      "  Batch 110/188, Loss: 1.2853\n",
      "  Batch 120/188, Loss: 1.7466\n",
      "  Batch 130/188, Loss: 1.5376\n",
      "  Batch 140/188, Loss: 1.2352\n",
      "  Batch 150/188, Loss: 1.8917\n",
      "  Batch 160/188, Loss: 1.6428\n",
      "  Batch 170/188, Loss: 1.4267\n",
      "  Batch 180/188, Loss: 1.7139\n",
      "Epoch 63 Loss: 1.4712\n",
      "  No improvement for 1 epochs\n",
      "Epoch 64/100 started\n",
      "  Batch 10/188, Loss: 1.5369\n",
      "  Batch 20/188, Loss: 1.2191\n",
      "  Batch 30/188, Loss: 1.1886\n",
      "  Batch 40/188, Loss: 1.4714\n",
      "  Batch 50/188, Loss: 1.2682\n",
      "  Batch 60/188, Loss: 1.2821\n",
      "  Batch 70/188, Loss: 0.9362\n",
      "  Batch 80/188, Loss: 1.5031\n",
      "  Batch 90/188, Loss: 1.4054\n",
      "  Batch 100/188, Loss: 1.6689\n",
      "  Batch 110/188, Loss: 1.1004\n",
      "  Batch 120/188, Loss: 1.5889\n",
      "  Batch 130/188, Loss: 1.6861\n",
      "  Batch 140/188, Loss: 2.0876\n",
      "  Batch 150/188, Loss: 1.4517\n",
      "  Batch 160/188, Loss: 1.4722\n",
      "  Batch 170/188, Loss: 1.7599\n",
      "  Batch 180/188, Loss: 1.1750\n",
      "Epoch 64 Loss: 1.4881\n",
      "  No improvement for 2 epochs\n",
      "Epoch 65/100 started\n",
      "  Batch 10/188, Loss: 1.3413\n",
      "  Batch 20/188, Loss: 1.6363\n",
      "  Batch 30/188, Loss: 1.4212\n",
      "  Batch 40/188, Loss: 1.9781\n",
      "  Batch 50/188, Loss: 1.5419\n",
      "  Batch 60/188, Loss: 1.5003\n",
      "  Batch 70/188, Loss: 1.5380\n",
      "  Batch 80/188, Loss: 1.4758\n",
      "  Batch 90/188, Loss: 2.0409\n",
      "  Batch 100/188, Loss: 1.7395\n",
      "  Batch 110/188, Loss: 1.5399\n",
      "  Batch 120/188, Loss: 1.7175\n",
      "  Batch 130/188, Loss: 1.4355\n",
      "  Batch 140/188, Loss: 1.6271\n",
      "  Batch 150/188, Loss: 1.6512\n",
      "  Batch 160/188, Loss: 1.4958\n",
      "  Batch 170/188, Loss: 1.6105\n",
      "  Batch 180/188, Loss: 1.2487\n",
      "Epoch 65 Loss: 1.4769\n",
      "  No improvement for 3 epochs\n",
      "Epoch 66/100 started\n",
      "  Batch 10/188, Loss: 0.9217\n",
      "  Batch 20/188, Loss: 1.3948\n",
      "  Batch 30/188, Loss: 1.0825\n",
      "  Batch 40/188, Loss: 1.5912\n",
      "  Batch 50/188, Loss: 1.3448\n",
      "  Batch 60/188, Loss: 1.4570\n",
      "  Batch 70/188, Loss: 1.6537\n",
      "  Batch 80/188, Loss: 1.0743\n",
      "  Batch 90/188, Loss: 1.3837\n",
      "  Batch 100/188, Loss: 1.6123\n",
      "  Batch 110/188, Loss: 1.8162\n",
      "  Batch 120/188, Loss: 1.2365\n",
      "  Batch 130/188, Loss: 1.2459\n",
      "  Batch 140/188, Loss: 1.8062\n",
      "  Batch 150/188, Loss: 1.5168\n",
      "  Batch 160/188, Loss: 1.4080\n",
      "  Batch 170/188, Loss: 1.4502\n",
      "  Batch 180/188, Loss: 1.4284\n",
      "Epoch 66 Loss: 1.4392\n",
      "  Model saved (improved loss: 1.4392)\n",
      "Epoch 67/100 started\n",
      "  Batch 10/188, Loss: 1.4607\n",
      "  Batch 20/188, Loss: 1.0490\n",
      "  Batch 30/188, Loss: 1.2133\n",
      "  Batch 40/188, Loss: 1.4751\n",
      "  Batch 50/188, Loss: 1.4926\n",
      "  Batch 60/188, Loss: 1.2657\n",
      "  Batch 70/188, Loss: 1.8045\n",
      "  Batch 80/188, Loss: 1.6095\n",
      "  Batch 90/188, Loss: 1.0521\n",
      "  Batch 100/188, Loss: 1.1041\n",
      "  Batch 110/188, Loss: 1.0866\n",
      "  Batch 120/188, Loss: 1.4594\n",
      "  Batch 130/188, Loss: 1.3822\n",
      "  Batch 140/188, Loss: 1.5722\n",
      "  Batch 150/188, Loss: 1.0357\n",
      "  Batch 160/188, Loss: 1.5552\n",
      "  Batch 170/188, Loss: 1.3273\n",
      "  Batch 180/188, Loss: 1.4140\n",
      "Epoch 67 Loss: 1.4311\n",
      "  Model saved (improved loss: 1.4311)\n",
      "Epoch 68/100 started\n",
      "  Batch 10/188, Loss: 1.3124\n",
      "  Batch 20/188, Loss: 1.2541\n",
      "  Batch 30/188, Loss: 1.6774\n",
      "  Batch 40/188, Loss: 1.1214\n",
      "  Batch 50/188, Loss: 1.2376\n",
      "  Batch 60/188, Loss: 1.6651\n",
      "  Batch 70/188, Loss: 1.0932\n",
      "  Batch 80/188, Loss: 0.9493\n",
      "  Batch 90/188, Loss: 2.0513\n",
      "  Batch 100/188, Loss: 1.0205\n",
      "  Batch 110/188, Loss: 1.2332\n",
      "  Batch 120/188, Loss: 1.2931\n",
      "  Batch 130/188, Loss: 1.7492\n",
      "  Batch 140/188, Loss: 1.8218\n",
      "  Batch 150/188, Loss: 1.3809\n",
      "  Batch 160/188, Loss: 1.2975\n",
      "  Batch 170/188, Loss: 1.4622\n",
      "  Batch 180/188, Loss: 1.6277\n",
      "Epoch 68 Loss: 1.4364\n",
      "  No improvement for 1 epochs\n",
      "Epoch 69/100 started\n",
      "  Batch 10/188, Loss: 1.2468\n",
      "  Batch 20/188, Loss: 1.1369\n",
      "  Batch 30/188, Loss: 1.3732\n",
      "  Batch 40/188, Loss: 1.2395\n",
      "  Batch 50/188, Loss: 1.5359\n",
      "  Batch 60/188, Loss: 1.4733\n",
      "  Batch 70/188, Loss: 1.2369\n",
      "  Batch 80/188, Loss: 1.2674\n",
      "  Batch 90/188, Loss: 1.9357\n",
      "  Batch 100/188, Loss: 1.3713\n",
      "  Batch 110/188, Loss: 1.4453\n",
      "  Batch 120/188, Loss: 1.4744\n",
      "  Batch 130/188, Loss: 1.5189\n",
      "  Batch 140/188, Loss: 2.1045\n",
      "  Batch 150/188, Loss: 1.0203\n",
      "  Batch 160/188, Loss: 1.1332\n",
      "  Batch 170/188, Loss: 1.1606\n",
      "  Batch 180/188, Loss: 1.3684\n",
      "Epoch 69 Loss: 1.4315\n",
      "  No improvement for 2 epochs\n",
      "Epoch 70/100 started\n",
      "  Batch 10/188, Loss: 1.7385\n",
      "  Batch 20/188, Loss: 1.2095\n",
      "  Batch 30/188, Loss: 1.7569\n",
      "  Batch 40/188, Loss: 1.2106\n",
      "  Batch 50/188, Loss: 1.5892\n",
      "  Batch 60/188, Loss: 1.6893\n",
      "  Batch 70/188, Loss: 1.1931\n",
      "  Batch 80/188, Loss: 1.3048\n",
      "  Batch 90/188, Loss: 1.1859\n",
      "  Batch 100/188, Loss: 1.3582\n",
      "  Batch 110/188, Loss: 1.3427\n",
      "  Batch 120/188, Loss: 1.4151\n",
      "  Batch 130/188, Loss: 1.7487\n",
      "  Batch 140/188, Loss: 1.1709\n",
      "  Batch 150/188, Loss: 1.6431\n",
      "  Batch 160/188, Loss: 1.5808\n",
      "  Batch 170/188, Loss: 1.3458\n",
      "  Batch 180/188, Loss: 1.5693\n",
      "Epoch 70 Loss: 1.4217\n",
      "  Model saved (improved loss: 1.4217)\n",
      "Epoch 71/100 started\n",
      "  Batch 10/188, Loss: 1.2557\n",
      "  Batch 20/188, Loss: 1.1732\n",
      "  Batch 30/188, Loss: 1.0129\n",
      "  Batch 40/188, Loss: 1.7485\n",
      "  Batch 50/188, Loss: 1.1132\n",
      "  Batch 60/188, Loss: 1.7018\n",
      "  Batch 70/188, Loss: 1.1476\n",
      "  Batch 80/188, Loss: 1.6051\n",
      "  Batch 90/188, Loss: 1.6221\n",
      "  Batch 100/188, Loss: 1.1218\n",
      "  Batch 110/188, Loss: 1.2698\n",
      "  Batch 120/188, Loss: 1.5924\n",
      "  Batch 130/188, Loss: 1.4938\n",
      "  Batch 140/188, Loss: 1.3657\n",
      "  Batch 150/188, Loss: 1.4790\n",
      "  Batch 160/188, Loss: 1.4459\n",
      "  Batch 170/188, Loss: 1.5491\n",
      "  Batch 180/188, Loss: 1.2710\n",
      "Epoch 71 Loss: 1.4018\n",
      "  Model saved (improved loss: 1.4018)\n",
      "Epoch 72/100 started\n",
      "  Batch 10/188, Loss: 0.9952\n",
      "  Batch 20/188, Loss: 0.6484\n",
      "  Batch 30/188, Loss: 1.4019\n",
      "  Batch 40/188, Loss: 1.2930\n",
      "  Batch 50/188, Loss: 1.6851\n",
      "  Batch 60/188, Loss: 1.1983\n",
      "  Batch 70/188, Loss: 1.4017\n",
      "  Batch 80/188, Loss: 1.4429\n",
      "  Batch 90/188, Loss: 1.2015\n",
      "  Batch 100/188, Loss: 1.8305\n",
      "  Batch 110/188, Loss: 0.8113\n",
      "  Batch 120/188, Loss: 0.7292\n",
      "  Batch 130/188, Loss: 1.3072\n",
      "  Batch 140/188, Loss: 1.3341\n",
      "  Batch 150/188, Loss: 1.2323\n",
      "  Batch 160/188, Loss: 1.6311\n",
      "  Batch 170/188, Loss: 2.1599\n",
      "  Batch 180/188, Loss: 1.4630\n",
      "Epoch 72 Loss: 1.4177\n",
      "  No improvement for 1 epochs\n",
      "Epoch 73/100 started\n",
      "  Batch 10/188, Loss: 1.4384\n",
      "  Batch 20/188, Loss: 1.5008\n",
      "  Batch 30/188, Loss: 1.4848\n",
      "  Batch 40/188, Loss: 1.3738\n",
      "  Batch 50/188, Loss: 1.3892\n",
      "  Batch 60/188, Loss: 0.8622\n",
      "  Batch 70/188, Loss: 1.3454\n",
      "  Batch 80/188, Loss: 0.9518\n",
      "  Batch 90/188, Loss: 1.3505\n",
      "  Batch 100/188, Loss: 1.3497\n",
      "  Batch 110/188, Loss: 1.6396\n",
      "  Batch 120/188, Loss: 1.6599\n",
      "  Batch 130/188, Loss: 1.1842\n",
      "  Batch 140/188, Loss: 1.0675\n",
      "  Batch 150/188, Loss: 1.5418\n",
      "  Batch 160/188, Loss: 1.6878\n",
      "  Batch 170/188, Loss: 1.5065\n",
      "  Batch 180/188, Loss: 1.8844\n",
      "Epoch 73 Loss: 1.4107\n",
      "  No improvement for 2 epochs\n",
      "Epoch 74/100 started\n",
      "  Batch 10/188, Loss: 1.5167\n",
      "  Batch 20/188, Loss: 1.9349\n",
      "  Batch 30/188, Loss: 1.6583\n",
      "  Batch 40/188, Loss: 1.3878\n",
      "  Batch 50/188, Loss: 1.6342\n",
      "  Batch 60/188, Loss: 1.1906\n",
      "  Batch 70/188, Loss: 1.2042\n",
      "  Batch 80/188, Loss: 1.5984\n",
      "  Batch 90/188, Loss: 1.3969\n",
      "  Batch 100/188, Loss: 1.6035\n",
      "  Batch 110/188, Loss: 1.7146\n",
      "  Batch 120/188, Loss: 1.4315\n",
      "  Batch 130/188, Loss: 1.4482\n",
      "  Batch 140/188, Loss: 1.3780\n",
      "  Batch 150/188, Loss: 1.4136\n",
      "  Batch 160/188, Loss: 1.0401\n",
      "  Batch 170/188, Loss: 1.3819\n",
      "  Batch 180/188, Loss: 1.1439\n",
      "Epoch 74 Loss: 1.3922\n",
      "  Model saved (improved loss: 1.3922)\n",
      "Epoch 75/100 started\n",
      "  Batch 10/188, Loss: 1.3360\n",
      "  Batch 20/188, Loss: 1.4937\n",
      "  Batch 30/188, Loss: 1.0849\n",
      "  Batch 40/188, Loss: 1.6038\n",
      "  Batch 50/188, Loss: 1.4707\n",
      "  Batch 60/188, Loss: 1.5556\n",
      "  Batch 70/188, Loss: 1.5177\n",
      "  Batch 80/188, Loss: 1.5654\n",
      "  Batch 90/188, Loss: 1.2727\n",
      "  Batch 100/188, Loss: 1.4204\n",
      "  Batch 110/188, Loss: 1.6638\n",
      "  Batch 120/188, Loss: 1.4734\n",
      "  Batch 130/188, Loss: 0.8590\n",
      "  Batch 140/188, Loss: 0.8604\n",
      "  Batch 150/188, Loss: 1.3784\n",
      "  Batch 160/188, Loss: 0.9613\n",
      "  Batch 170/188, Loss: 1.5553\n",
      "  Batch 180/188, Loss: 1.0599\n",
      "Epoch 75 Loss: 1.3790\n",
      "  Model saved (improved loss: 1.3790)\n",
      "Epoch 76/100 started\n",
      "  Batch 10/188, Loss: 1.0060\n",
      "  Batch 20/188, Loss: 1.6690\n",
      "  Batch 30/188, Loss: 1.0774\n",
      "  Batch 40/188, Loss: 1.8419\n",
      "  Batch 50/188, Loss: 1.2314\n",
      "  Batch 60/188, Loss: 1.1388\n",
      "  Batch 70/188, Loss: 1.1980\n",
      "  Batch 80/188, Loss: 1.3519\n",
      "  Batch 90/188, Loss: 1.2856\n",
      "  Batch 100/188, Loss: 1.1775\n",
      "  Batch 110/188, Loss: 1.1691\n",
      "  Batch 120/188, Loss: 1.7997\n",
      "  Batch 130/188, Loss: 1.6169\n",
      "  Batch 140/188, Loss: 1.5267\n",
      "  Batch 150/188, Loss: 2.0181\n",
      "  Batch 160/188, Loss: 1.1410\n",
      "  Batch 170/188, Loss: 1.5759\n",
      "  Batch 180/188, Loss: 1.3060\n",
      "Epoch 76 Loss: 1.4031\n",
      "  No improvement for 1 epochs\n",
      "Epoch 77/100 started\n",
      "  Batch 10/188, Loss: 1.4988\n",
      "  Batch 20/188, Loss: 1.5075\n",
      "  Batch 30/188, Loss: 1.5971\n",
      "  Batch 40/188, Loss: 1.2767\n",
      "  Batch 50/188, Loss: 1.2319\n",
      "  Batch 60/188, Loss: 1.3199\n",
      "  Batch 70/188, Loss: 1.5421\n",
      "  Batch 80/188, Loss: 1.1475\n",
      "  Batch 90/188, Loss: 1.2351\n",
      "  Batch 100/188, Loss: 1.2225\n",
      "  Batch 110/188, Loss: 1.5843\n",
      "  Batch 120/188, Loss: 1.3316\n",
      "  Batch 130/188, Loss: 1.0525\n",
      "  Batch 140/188, Loss: 1.2731\n",
      "  Batch 150/188, Loss: 0.9332\n",
      "  Batch 160/188, Loss: 1.1817\n",
      "  Batch 170/188, Loss: 1.6409\n",
      "  Batch 180/188, Loss: 1.6251\n",
      "Epoch 77 Loss: 1.3782\n",
      "  Model saved (improved loss: 1.3782)\n",
      "Epoch 78/100 started\n",
      "  Batch 10/188, Loss: 1.7503\n",
      "  Batch 20/188, Loss: 1.1693\n",
      "  Batch 30/188, Loss: 1.2247\n",
      "  Batch 40/188, Loss: 1.4345\n",
      "  Batch 50/188, Loss: 1.2022\n",
      "  Batch 60/188, Loss: 0.7843\n",
      "  Batch 70/188, Loss: 1.9028\n",
      "  Batch 80/188, Loss: 1.2129\n",
      "  Batch 90/188, Loss: 1.1705\n",
      "  Batch 100/188, Loss: 1.2138\n",
      "  Batch 110/188, Loss: 1.6381\n",
      "  Batch 120/188, Loss: 1.2258\n",
      "  Batch 130/188, Loss: 1.3474\n",
      "  Batch 140/188, Loss: 1.5341\n",
      "  Batch 150/188, Loss: 1.7694\n",
      "  Batch 160/188, Loss: 1.1581\n",
      "  Batch 170/188, Loss: 1.2543\n",
      "  Batch 180/188, Loss: 1.4019\n",
      "Epoch 78 Loss: 1.3704\n",
      "  Model saved (improved loss: 1.3704)\n",
      "Epoch 79/100 started\n",
      "  Batch 10/188, Loss: 1.0144\n",
      "  Batch 20/188, Loss: 1.2426\n",
      "  Batch 30/188, Loss: 1.0570\n",
      "  Batch 40/188, Loss: 1.4546\n",
      "  Batch 50/188, Loss: 1.0695\n",
      "  Batch 60/188, Loss: 1.6722\n",
      "  Batch 70/188, Loss: 1.4599\n",
      "  Batch 80/188, Loss: 1.5743\n",
      "  Batch 90/188, Loss: 1.7987\n",
      "  Batch 100/188, Loss: 1.6892\n",
      "  Batch 110/188, Loss: 1.8413\n",
      "  Batch 120/188, Loss: 1.1826\n",
      "  Batch 130/188, Loss: 1.2792\n",
      "  Batch 140/188, Loss: 1.5779\n",
      "  Batch 150/188, Loss: 1.3912\n",
      "  Batch 160/188, Loss: 1.5157\n",
      "  Batch 170/188, Loss: 1.6555\n",
      "  Batch 180/188, Loss: 1.4082\n",
      "Epoch 79 Loss: 1.3517\n",
      "  Model saved (improved loss: 1.3517)\n",
      "Epoch 80/100 started\n",
      "  Batch 10/188, Loss: 1.1735\n",
      "  Batch 20/188, Loss: 1.5315\n",
      "  Batch 30/188, Loss: 1.4595\n",
      "  Batch 40/188, Loss: 1.0662\n",
      "  Batch 50/188, Loss: 1.3898\n",
      "  Batch 60/188, Loss: 1.1745\n",
      "  Batch 70/188, Loss: 1.0702\n",
      "  Batch 80/188, Loss: 1.3553\n",
      "  Batch 90/188, Loss: 1.9498\n",
      "  Batch 100/188, Loss: 1.0302\n",
      "  Batch 110/188, Loss: 1.2805\n",
      "  Batch 120/188, Loss: 1.1823\n",
      "  Batch 130/188, Loss: 0.9995\n",
      "  Batch 140/188, Loss: 1.8333\n",
      "  Batch 150/188, Loss: 1.5939\n",
      "  Batch 160/188, Loss: 1.2062\n",
      "  Batch 170/188, Loss: 1.1936\n",
      "  Batch 180/188, Loss: 1.2067\n",
      "Epoch 80 Loss: 1.3681\n",
      "  No improvement for 1 epochs\n",
      "Epoch 81/100 started\n",
      "  Batch 10/188, Loss: 1.3243\n",
      "  Batch 20/188, Loss: 1.2048\n",
      "  Batch 30/188, Loss: 1.2788\n",
      "  Batch 40/188, Loss: 1.3722\n",
      "  Batch 50/188, Loss: 1.4939\n",
      "  Batch 60/188, Loss: 1.4515\n",
      "  Batch 70/188, Loss: 1.9044\n",
      "  Batch 80/188, Loss: 1.1452\n",
      "  Batch 90/188, Loss: 1.4896\n",
      "  Batch 100/188, Loss: 0.5077\n",
      "  Batch 110/188, Loss: 1.2907\n",
      "  Batch 120/188, Loss: 1.4308\n",
      "  Batch 130/188, Loss: 1.6957\n",
      "  Batch 140/188, Loss: 1.2189\n",
      "  Batch 150/188, Loss: 2.1596\n",
      "  Batch 160/188, Loss: 1.6680\n",
      "  Batch 170/188, Loss: 1.2817\n",
      "  Batch 180/188, Loss: 2.1639\n",
      "Epoch 81 Loss: 1.3511\n",
      "  Model saved (improved loss: 1.3511)\n",
      "Epoch 82/100 started\n",
      "  Batch 10/188, Loss: 1.2234\n",
      "  Batch 20/188, Loss: 1.5078\n",
      "  Batch 30/188, Loss: 1.5663\n",
      "  Batch 40/188, Loss: 1.2028\n",
      "  Batch 50/188, Loss: 1.7581\n",
      "  Batch 60/188, Loss: 1.7215\n",
      "  Batch 70/188, Loss: 1.5518\n",
      "  Batch 80/188, Loss: 0.8772\n",
      "  Batch 90/188, Loss: 1.4466\n",
      "  Batch 100/188, Loss: 1.2063\n",
      "  Batch 110/188, Loss: 1.3486\n",
      "  Batch 120/188, Loss: 1.7350\n",
      "  Batch 130/188, Loss: 0.8998\n",
      "  Batch 140/188, Loss: 1.4865\n",
      "  Batch 150/188, Loss: 1.2256\n",
      "  Batch 160/188, Loss: 0.7137\n",
      "  Batch 170/188, Loss: 1.1144\n",
      "  Batch 180/188, Loss: 1.3252\n",
      "Epoch 82 Loss: 1.3527\n",
      "  No improvement for 1 epochs\n",
      "Epoch 83/100 started\n",
      "  Batch 10/188, Loss: 1.2519\n",
      "  Batch 20/188, Loss: 1.5108\n",
      "  Batch 30/188, Loss: 1.4333\n",
      "  Batch 40/188, Loss: 1.5131\n",
      "  Batch 50/188, Loss: 1.3379\n",
      "  Batch 60/188, Loss: 1.0265\n",
      "  Batch 70/188, Loss: 1.5691\n",
      "  Batch 80/188, Loss: 0.9787\n",
      "  Batch 90/188, Loss: 1.1253\n",
      "  Batch 100/188, Loss: 1.2652\n",
      "  Batch 110/188, Loss: 1.1513\n",
      "  Batch 120/188, Loss: 1.2612\n",
      "  Batch 130/188, Loss: 1.1409\n",
      "  Batch 140/188, Loss: 1.1429\n",
      "  Batch 150/188, Loss: 1.3363\n",
      "  Batch 160/188, Loss: 1.5879\n",
      "  Batch 170/188, Loss: 1.5266\n",
      "  Batch 180/188, Loss: 1.5538\n",
      "Epoch 83 Loss: 1.3210\n",
      "  Model saved (improved loss: 1.3210)\n",
      "Epoch 84/100 started\n",
      "  Batch 10/188, Loss: 1.4537\n",
      "  Batch 20/188, Loss: 1.4498\n",
      "  Batch 30/188, Loss: 0.6204\n",
      "  Batch 40/188, Loss: 1.5083\n",
      "  Batch 50/188, Loss: 1.4332\n",
      "  Batch 60/188, Loss: 1.3213\n",
      "  Batch 70/188, Loss: 1.2842\n",
      "  Batch 80/188, Loss: 1.8205\n",
      "  Batch 90/188, Loss: 1.0298\n",
      "  Batch 100/188, Loss: 1.1124\n",
      "  Batch 110/188, Loss: 1.3177\n",
      "  Batch 120/188, Loss: 1.2449\n",
      "  Batch 130/188, Loss: 1.6762\n",
      "  Batch 140/188, Loss: 1.1364\n",
      "  Batch 150/188, Loss: 1.2384\n",
      "  Batch 160/188, Loss: 1.4807\n",
      "  Batch 170/188, Loss: 1.4178\n",
      "  Batch 180/188, Loss: 1.3815\n",
      "Epoch 84 Loss: 1.3404\n",
      "  No improvement for 1 epochs\n",
      "Epoch 85/100 started\n",
      "  Batch 10/188, Loss: 1.4619\n",
      "  Batch 20/188, Loss: 1.3548\n",
      "  Batch 30/188, Loss: 1.3119\n",
      "  Batch 40/188, Loss: 0.9480\n",
      "  Batch 50/188, Loss: 1.2997\n",
      "  Batch 60/188, Loss: 1.3410\n",
      "  Batch 70/188, Loss: 1.5103\n",
      "  Batch 80/188, Loss: 1.4790\n",
      "  Batch 90/188, Loss: 1.1699\n",
      "  Batch 100/188, Loss: 1.6779\n",
      "  Batch 110/188, Loss: 1.3804\n",
      "  Batch 120/188, Loss: 0.9695\n",
      "  Batch 130/188, Loss: 1.3078\n",
      "  Batch 140/188, Loss: 1.9820\n",
      "  Batch 150/188, Loss: 1.7391\n",
      "  Batch 160/188, Loss: 1.4048\n",
      "  Batch 170/188, Loss: 1.7221\n",
      "  Batch 180/188, Loss: 1.3216\n",
      "Epoch 85 Loss: 1.3495\n",
      "  No improvement for 2 epochs\n",
      "Epoch 86/100 started\n",
      "  Batch 10/188, Loss: 1.4563\n",
      "  Batch 20/188, Loss: 1.2693\n",
      "  Batch 30/188, Loss: 1.2492\n",
      "  Batch 40/188, Loss: 1.7057\n",
      "  Batch 50/188, Loss: 1.2860\n",
      "  Batch 60/188, Loss: 1.2837\n",
      "  Batch 70/188, Loss: 0.8944\n",
      "  Batch 80/188, Loss: 1.5254\n",
      "  Batch 90/188, Loss: 1.3730\n",
      "  Batch 100/188, Loss: 1.0919\n",
      "  Batch 110/188, Loss: 1.4254\n",
      "  Batch 120/188, Loss: 1.3724\n",
      "  Batch 130/188, Loss: 1.0101\n",
      "  Batch 140/188, Loss: 1.4061\n",
      "  Batch 150/188, Loss: 1.9443\n",
      "  Batch 160/188, Loss: 1.1692\n",
      "  Batch 170/188, Loss: 1.3822\n",
      "  Batch 180/188, Loss: 0.9762\n",
      "Epoch 86 Loss: 1.3108\n",
      "  Model saved (improved loss: 1.3108)\n",
      "Epoch 87/100 started\n",
      "  Batch 10/188, Loss: 0.8578\n",
      "  Batch 20/188, Loss: 0.9281\n",
      "  Batch 30/188, Loss: 1.3063\n",
      "  Batch 40/188, Loss: 1.1468\n",
      "  Batch 50/188, Loss: 1.5162\n",
      "  Batch 60/188, Loss: 1.4178\n",
      "  Batch 70/188, Loss: 1.2315\n",
      "  Batch 80/188, Loss: 1.0614\n",
      "  Batch 90/188, Loss: 1.3271\n",
      "  Batch 100/188, Loss: 1.7809\n",
      "  Batch 110/188, Loss: 1.1665\n",
      "  Batch 120/188, Loss: 1.1871\n",
      "  Batch 130/188, Loss: 1.5788\n",
      "  Batch 140/188, Loss: 1.2539\n",
      "  Batch 150/188, Loss: 1.7056\n",
      "  Batch 160/188, Loss: 1.3132\n",
      "  Batch 170/188, Loss: 0.8929\n",
      "  Batch 180/188, Loss: 1.0972\n",
      "Epoch 87 Loss: 1.3416\n",
      "  No improvement for 1 epochs\n",
      "Epoch 88/100 started\n",
      "  Batch 10/188, Loss: 1.3528\n",
      "  Batch 20/188, Loss: 0.9353\n",
      "  Batch 30/188, Loss: 1.1350\n",
      "  Batch 40/188, Loss: 1.2263\n",
      "  Batch 50/188, Loss: 1.2213\n",
      "  Batch 60/188, Loss: 1.2638\n",
      "  Batch 70/188, Loss: 1.0662\n",
      "  Batch 80/188, Loss: 1.5612\n",
      "  Batch 90/188, Loss: 1.4441\n",
      "  Batch 100/188, Loss: 1.5575\n",
      "  Batch 110/188, Loss: 1.4014\n",
      "  Batch 120/188, Loss: 1.2553\n",
      "  Batch 130/188, Loss: 1.3927\n",
      "  Batch 140/188, Loss: 1.7671\n",
      "  Batch 150/188, Loss: 1.6869\n",
      "  Batch 160/188, Loss: 1.0443\n",
      "  Batch 170/188, Loss: 1.6087\n",
      "  Batch 180/188, Loss: 1.5120\n",
      "Epoch 88 Loss: 1.3302\n",
      "  No improvement for 2 epochs\n",
      "Epoch 89/100 started\n",
      "  Batch 10/188, Loss: 1.2081\n",
      "  Batch 20/188, Loss: 1.5250\n",
      "  Batch 30/188, Loss: 1.1028\n",
      "  Batch 40/188, Loss: 1.2512\n",
      "  Batch 50/188, Loss: 1.3677\n",
      "  Batch 60/188, Loss: 0.8712\n",
      "  Batch 70/188, Loss: 1.4496\n",
      "  Batch 80/188, Loss: 1.2922\n",
      "  Batch 90/188, Loss: 1.0417\n",
      "  Batch 100/188, Loss: 1.0679\n",
      "  Batch 110/188, Loss: 1.4537\n",
      "  Batch 120/188, Loss: 1.1615\n",
      "  Batch 130/188, Loss: 1.4881\n",
      "  Batch 140/188, Loss: 1.2504\n",
      "  Batch 150/188, Loss: 1.1363\n",
      "  Batch 160/188, Loss: 1.6018\n",
      "  Batch 170/188, Loss: 1.4806\n",
      "  Batch 180/188, Loss: 2.0555\n",
      "Epoch 89 Loss: 1.3316\n",
      "  No improvement for 3 epochs\n",
      "Epoch 90/100 started\n",
      "  Batch 10/188, Loss: 1.4434\n",
      "  Batch 20/188, Loss: 1.2659\n",
      "  Batch 30/188, Loss: 1.3846\n",
      "  Batch 40/188, Loss: 1.3655\n",
      "  Batch 50/188, Loss: 1.1198\n",
      "  Batch 60/188, Loss: 1.6388\n",
      "  Batch 70/188, Loss: 1.7714\n",
      "  Batch 80/188, Loss: 1.3990\n",
      "  Batch 90/188, Loss: 1.2902\n",
      "  Batch 100/188, Loss: 0.9310\n",
      "  Batch 110/188, Loss: 1.2443\n",
      "  Batch 120/188, Loss: 1.0882\n",
      "  Batch 130/188, Loss: 0.9917\n",
      "  Batch 140/188, Loss: 1.5521\n",
      "  Batch 150/188, Loss: 0.9598\n",
      "  Batch 160/188, Loss: 1.5068\n",
      "  Batch 170/188, Loss: 1.1545\n",
      "  Batch 180/188, Loss: 0.9440\n",
      "Epoch 90 Loss: 1.3264\n",
      "  No improvement for 4 epochs\n",
      "Early stopping triggered.\n",
      "Final model saved.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "best_loss = float('inf')\n",
    "patience = 4\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} started\")\n",
    "    try:\n",
    "        epoch_loss = train_epoch(model, train_loader, optimizer, accumulation_steps=4)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Check improvement\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), 'card_classifier_best_224.pth')\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"  Model saved (improved loss: {best_loss:.4f})\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"  No improvement for {no_improve_epochs} epochs\")\n",
    "        \n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "        break\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'card_classifier_224_50.pth')\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delte this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "card",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
